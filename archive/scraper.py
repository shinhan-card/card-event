"""
ì›¹ í¬ë¡¤ë§ ëª¨ë“ˆ
Playwright Stealthë¥¼ í™œìš©í•œ ì¹´ë“œì‚¬ ì´ë²¤íŠ¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
ê°•ë ¥í•œ ë´‡ íƒì§€ íšŒí”¼ ê¸°ëŠ¥
"""

from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import stealth_async
from bs4 import BeautifulSoup
import random
import asyncio
import os
from typing import List, Dict, Tuple
from dotenv import load_dotenv

load_dotenv()


class CardEventScraper:
    """ì¹´ë“œì‚¬ ì´ë²¤íŠ¸ ìŠ¤í¬ë˜í¼ (Stealth ëª¨ë“œ)"""
    
    # ìµœì‹  í¬ë¡¬ ë²„ì „ User-Agents (2026ë…„ 2ì›” ê¸°ì¤€)
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
    ]
    
    def __init__(self):
        """ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”"""
        self.browser: Browser = None
        self.page: Page = None
        
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì¹´ë“œì‚¬ URL ë¡œë“œ
        self.card_companies = {
            "ì‹ í•œì¹´ë“œ": os.getenv("SHINHAN_EVENT_URL", "https://www.shinhancard.com/pconts/html/benefit/event/main.html"),
            "ì‚¼ì„±ì¹´ë“œ": os.getenv("SAMSUNG_EVENT_URL", "https://www.samsungcard.com/personal/benefit/event/list.do"),
            "í˜„ëŒ€ì¹´ë“œ": os.getenv("HYUNDAI_EVENT_URL", "https://www.hyundaicard.com/event/eventlist.hdc"),
            "KBêµ­ë¯¼ì¹´ë“œ": os.getenv("KB_EVENT_URL", "https://www.kbcard.com/CRD/DVIEW/MBCXBDDAMBC0001.do"),
        }
    
    async def init_browser(self, headless: bool = True):
        """
        Playwright Stealth ì ìš© ë¸Œë¼ìš°ì € ì´ˆê¸°í™”
        - playwright-stealthë¡œ ìë™í™” íƒì§€ ì™„ë²½ íšŒí”¼
        - headless/non-headless ëª¨ë‘ ëŒ€ì‘
        """
        playwright = await async_playwright().start()
        
        # ë¸Œë¼ìš°ì € ì‹¤í–‰ ì˜µì…˜
        launch_args = [
            '--disable-blink-features=AutomationControlled',
            '--disable-dev-shm-usage',
            '--disable-setuid-sandbox',
            '--no-first-run',
            '--no-default-browser-check',
            '--disable-infobars',
            '--window-size=1920,1080',
        ]
        
        if headless:
            launch_args.extend([
                '--disable-gpu',
                '--disable-software-rasterizer',
            ])
        
        self.browser = await playwright.chromium.launch(
            headless=headless,
            args=launch_args,
            chromium_sandbox=False
        )
        
        # ì‹¤ì œ ì‚¬ìš©ìì²˜ëŸ¼ ë³´ì´ëŠ” ì»¨í…ìŠ¤íŠ¸
        selected_ua = random.choice(self.USER_AGENTS)
        
        context = await self.browser.new_context(
            user_agent=selected_ua,
            viewport={'width': 1920, 'height': 1080},
            locale='ko-KR',
            timezone_id='Asia/Seoul',
            permissions=['geolocation'],
            screen={'width': 1920, 'height': 1080},
            java_script_enabled=True,
        )
        
        # ê°•ë ¥í•œ ìë™í™” íƒì§€ íšŒí”¼ ìŠ¤í¬ë¦½íŠ¸
        await context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
            
            window.chrome = {
                runtime: {},
                loadTimes: function() {},
                csi: function() {},
                app: {}
            };
            
            const originalQuery = window.navigator.permissions.query;
            window.navigator.permissions.query = (parameters) => (
                parameters.name === 'notifications' ?
                    Promise.resolve({ state: Notification.permission }) :
                    originalQuery(parameters)
            );
            
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5]
            });
            
            Object.defineProperty(navigator, 'languages', {
                get: () => ['ko-KR', 'ko', 'en-US', 'en']
            });
        """)
        
        self.page = await context.new_page()
        
        # â­ playwright-stealth ì ìš©!
        await stealth_async(self.page)
        
        print("âœ… ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ (Stealth ëª¨ë“œ ì ìš©)")
    
    async def close_browser(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        if self.browser:
            await self.browser.close()
            print("ğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ")
    
    async def random_delay(self, min_sec: float = 1.0, max_sec: float = 3.0):
        """ëœë¤ ëŒ€ê¸° (ë´‡ íƒì§€ ìš°íšŒ)"""
        delay = random.uniform(min_sec, max_sec)
        await asyncio.sleep(delay)
    
    async def human_like_scroll(self):
        """ì‚¬ëŒì²˜ëŸ¼ ìŠ¤í¬ë¡¤"""
        scroll_count = random.randint(3, 7)
        for _ in range(scroll_count):
            scroll_amount = random.randint(300, 800)
            await self.page.evaluate(f"window.scrollBy(0, {scroll_amount})")
            await asyncio.sleep(random.uniform(0.3, 0.8))
        
        if random.random() > 0.5:
            await self.page.evaluate("window.scrollTo(0, 0)")
            await asyncio.sleep(random.uniform(0.5, 1.0))
    
    async def human_like_mouse_move(self):
        """ì‚¬ëŒì²˜ëŸ¼ ë§ˆìš°ìŠ¤ ì´ë™"""
        try:
            x = random.randint(100, 1800)
            y = random.randint(100, 900)
            await self.page.mouse.move(x, y)
            await asyncio.sleep(random.uniform(0.1, 0.3))
        except:
            pass
    
    async def get_page_content(self, url: str) -> str:
        """í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸°"""
        try:
            await self.page.goto(url, wait_until="networkidle", timeout=30000)
            await self.random_delay(2, 4)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # ìŠ¤í¬ë¡¤ ë‹¤ìš´ (ë™ì  ì½˜í…ì¸  ë¡œë”©)
            await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await self.random_delay(1, 2)
            
            content = await self.page.content()
            return content
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨ ({url}): {e}")
            return ""
    
    async def extract_event_list_urls(self, company: str, list_url: str) -> List[str]:
        """
        ì´ë²¤íŠ¸ ëª©ë¡ í˜ì´ì§€ì—ì„œ ìƒì„¸ URL ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)
        
        Args:
            company: ì¹´ë“œì‚¬ëª…
            list_url: ì´ë²¤íŠ¸ ëª©ë¡ í˜ì´ì§€ URL
        
        Returns:
            list: ì´ë²¤íŠ¸ ìƒì„¸ URL ë¦¬ìŠ¤íŠ¸
        """
        print(f"\nğŸ“‹ [{company}] ì´ë²¤íŠ¸ ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
        
        try:
            # Playwrightë¡œ í˜ì´ì§€ ì§ì ‘ ì¡°ì‘
            await self.page.goto(list_url, wait_until="networkidle", timeout=30000)
            await self.random_delay(2, 4)
            
            # ìŠ¤í¬ë¡¤í•˜ì—¬ ë™ì  ì½˜í…ì¸  ë¡œë”©
            for _ in range(3):
                await self.page.evaluate("window.scrollBy(0, window.innerHeight)")
                await self.random_delay(1, 2)
            
            event_urls = []
            
            # Playwrightë¡œ ë§í¬ ì§ì ‘ ì¶”ì¶œ (ë” ì•ˆì •ì )
            if company == "ì‹ í•œì¹´ë“œ":
                # ì‹ í•œì¹´ë“œ: li.event-item > a ë˜ëŠ” .event-list a íŒ¨í„´
                selectors = [
                    'a[href*="eventDetail"]',
                    'a[href*="/event/"]',
                    '.event-list a',
                    'li.event-item a',
                    'div.event-box a',
                    'a[onclick*="event"]'
                ]
            elif company == "ì‚¼ì„±ì¹´ë“œ":
                # ì‚¼ì„±ì¹´ë“œ: benefit/event ê²½ë¡œ
                selectors = [
                    'a[href*="eventDetail"]',
                    'a[href*="benefit/event"]',
                    '.event-item a',
                    'div.benefit-list a',
                    'a[onclick*="goDetail"]'
                ]
            elif company == "í˜„ëŒ€ì¹´ë“œ":
                # í˜„ëŒ€ì¹´ë“œ: eventView, eventDetail íŒ¨í„´
                selectors = [
                    'a[href*="eventView"]',
                    'a[href*="eventDetail"]',
                    'a[href*="/event/"]',
                    '.event-card a',
                    'li.list-item a'
                ]
            elif company == "KBêµ­ë¯¼ì¹´ë“œ":
                # KBêµ­ë¯¼ì¹´ë“œ: DVIEW, event íŒ¨í„´
                selectors = [
                    'a[href*="DVIEW"]',
                    'a[href*="event"]',
                    'a[href*="benefit"]',
                    '.card-list a',
                    'div.event-item a'
                ]
            else:
                # ë²”ìš© íŒ¨í„´
                selectors = [
                    'a[href*="event"]',
                    'a[href*="Event"]',
                    'a[href*="benefit"]'
                ]
            
            # ê° ì…€ë ‰í„° ì‹œë„
            for selector in selectors:
                try:
                    links = await self.page.query_selector_all(selector)
                    print(f"  [DEBUG] {selector}: {len(links)}ê°œ ë°œê²¬")
                    
                    for link in links[:30]:  # ê° ì…€ë ‰í„°ë‹¹ ìµœëŒ€ 30ê°œ
                        href = await link.get_attribute('href')
                        if href:
                            # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                            if href.startswith('/'):
                                from urllib.parse import urljoin
                                full_url = urljoin(list_url, href)
                            elif href.startswith('http'):
                                full_url = href
                            elif href.startswith('javascript:') or href.startswith('#'):
                                # onclick ì´ë²¤íŠ¸ì—ì„œ URL ì¶”ì¶œ ì‹œë„
                                onclick = await link.get_attribute('onclick')
                                if onclick and ('event' in onclick.lower() or 'detail' in onclick.lower()):
                                    # onclickì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ (ì˜ˆ: goDetail('123'))
                                    import re
                                    match = re.search(r"['\"]([^'\"]+)['\"]", onclick)
                                    if match:
                                        param = match.group(1)
                                        full_url = f"{list_url}?id={param}"
                                    else:
                                        continue
                                else:
                                    continue
                            else:
                                # ê¸°íƒ€ ìƒëŒ€ ê²½ë¡œ
                                from urllib.parse import urljoin
                                full_url = urljoin(list_url, href)
                            
                            # ì¤‘ë³µ ì œê±° ë° ìœ íš¨ì„± ê²€ì‚¬
                            if full_url not in event_urls and len(full_url) > 10:
                                # ê°™ì€ ëª©ë¡ í˜ì´ì§€ëŠ” ì œì™¸
                                if full_url != list_url and 'list' not in full_url.split('/')[-1].lower():
                                    event_urls.append(full_url)
                
                except Exception as e:
                    print(f"  [WARN] {selector} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            # ì¤‘ë³µ ì œê±°
            event_urls = list(dict.fromkeys(event_urls))[:20]  # ìµœëŒ€ 20ê°œ
            
            print(f"âœ… [{company}] ì´ {len(event_urls)}ê°œ ì´ë²¤íŠ¸ URL ë°œê²¬")
            
            if event_urls:
                print(f"  ìƒ˜í”Œ URL: {event_urls[0][:80]}...")
            
            return event_urls
            
        except Exception as e:
            print(f"âŒ [{company}] ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    async def extract_event_detail(self, url: str) -> Tuple[str, str]:
        """
        ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)
        
        Args:
            url: ì´ë²¤íŠ¸ ìƒì„¸ URL
        
        Returns:
            tuple: (url, ì¶”ì¶œëœ í…ìŠ¤íŠ¸)
        """
        print(f"ğŸ“„ ìƒì„¸ í˜ì´ì§€ ë¶„ì„ ì¤‘: {url[:60]}...")
        
        try:
            await self.page.goto(url, wait_until="networkidle", timeout=30000)
            await self.random_delay(2, 3)
            
            # í˜ì´ì§€ ìŠ¤í¬ë¡¤ (lazy loading ì»¨í…ì¸  ë¡œë“œ)
            await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await self.random_delay(1, 2)
            
            html = await self.page.content()
            soup = BeautifulSoup(html, 'lxml')
            
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe', 'noscript']):
                tag.decompose()
            
            text_parts = []
            
            # 1. ì œëª© ì¶”ì¶œ (ìš°ì„ ìˆœìœ„ ìˆœ)
            title_selectors = [
                'h1.event-title', 'h1.title', 'div.event-title h1',
                'h2.event-title', 'h2.title', 'div.title h2',
                'h1', 'h2', '.page-title', 'title'
            ]
            
            for selector in title_selectors:
                title = soup.select_one(selector)
                if title:
                    title_text = title.get_text(strip=True)
                    if len(title_text) > 3:  # ìµœì†Œ 3ê¸€ì ì´ìƒ
                        text_parts.append(f"[ì œëª©] {title_text}")
                        break
            
            # 2. ì´ë²¤íŠ¸ ê¸°ê°„ ì¶”ì¶œ
            period_selectors = [
                '.event-period', '.period', '.date', '.event-date',
                'span:contains("ê¸°ê°„")', 'div:contains("ì´ë²¤íŠ¸ ê¸°ê°„")',
                'p.date', 'div.period'
            ]
            
            for selector in period_selectors:
                try:
                    period = soup.select_one(selector)
                    if period:
                        period_text = period.get_text(strip=True)
                        if any(char in period_text for char in ['~', '-', 'ë¶€í„°', 'ê¹Œì§€', 'ê¸°ê°„']):
                            text_parts.append(f"[ê¸°ê°„] {period_text}")
                            break
                except:
                    continue
            
            # 3. ë³¸ë¬¸ ì½˜í…ì¸  ì¶”ì¶œ (ìš°ì„ ìˆœìœ„ ìˆœ)
            content_selectors = [
                'div.event-content', 'div.event-detail', 'article.event',
                'div.detail-content', 'div.content', 'article',
                'main', 'div.container', 'div#content'
            ]
            
            content_found = False
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    # ì¤‘ì²©ëœ ë¦¬ìŠ¤íŠ¸ë‚˜ í…Œì´ë¸”ë„ ì²˜ë¦¬
                    content_text = content.get_text(separator='\n', strip=True)
                    if len(content_text) > 50:  # ìµœì†Œ 50ì ì´ìƒ
                        text_parts.append(f"[ë³¸ë¬¸]\n{content_text}")
                        content_found = True
                        break
            
            # ë³¸ë¬¸ì„ ëª» ì°¾ì•˜ìœ¼ë©´ body ì „ì²´
            if not content_found:
                body = soup.find('body')
                if body:
                    body_text = body.get_text(separator='\n', strip=True)
                    text_parts.append(f"[ë³¸ë¬¸]\n{body_text}")
            
            # 4. í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ (í˜œíƒ ì •ë³´ê°€ í‘œ í˜•ì‹ì¸ ê²½ìš°)
            tables = soup.find_all('table')
            for i, table in enumerate(tables[:3], 1):  # ìµœëŒ€ 3ê°œ í…Œì´ë¸”
                try:
                    rows = table.find_all('tr')
                    table_text = []
                    for row in rows:
                        cells = [cell.get_text(strip=True) for cell in row.find_all(['th', 'td'])]
                        if cells:
                            table_text.append(' | '.join(cells))
                    if table_text:
                        text_parts.append(f"[í‘œ {i}]\n" + '\n'.join(table_text))
                except:
                    continue
            
            # 5. ë¦¬ìŠ¤íŠ¸ í•­ëª© ì¶”ì¶œ (í˜œíƒ ì¡°ê±´ ë“±)
            lists = soup.find_all(['ul', 'ol'])
            for i, ul in enumerate(lists[:5], 1):  # ìµœëŒ€ 5ê°œ ë¦¬ìŠ¤íŠ¸
                try:
                    items = ul.find_all('li')
                    if len(items) > 0 and len(items) < 30:  # ë„ˆë¬´ ë§ìœ¼ë©´ ë©”ë‰´ì¼ ê°€ëŠ¥ì„±
                        list_text = []
                        for item in items:
                            item_text = item.get_text(strip=True)
                            if len(item_text) > 5:
                                list_text.append(f"  - {item_text}")
                        if list_text:
                            text_parts.append(f"[ëª©ë¡ {i}]\n" + '\n'.join(list_text))
                except:
                    continue
            
            # 6. ì´ë¯¸ì§€ alt ì†ì„± (í˜œíƒì´ ì´ë¯¸ì§€ë¡œ í‘œí˜„ëœ ê²½ìš°)
            images = soup.find_all('img', alt=True)
            alt_texts = []
            for img in images[:15]:
                alt_text = img.get('alt', '').strip()
                if alt_text and len(alt_text) > 5 and 'ë¡œê³ ' not in alt_text and 'logo' not in alt_text.lower():
                    alt_texts.append(f"  - {alt_text}")
            
            if alt_texts:
                text_parts.append(f"[ì´ë¯¸ì§€ ì„¤ëª…]\n" + '\n'.join(alt_texts))
            
            # 7. ë©”íƒ€ íƒœê·¸ì—ì„œ ì„¤ëª… ì¶”ì¶œ
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc and meta_desc.get('content'):
                text_parts.append(f"[ë©”íƒ€ ì„¤ëª…] {meta_desc['content']}")
            
            # í…ìŠ¤íŠ¸ ì •ì œ
            full_text = '\n\n'.join(text_parts)
            
            # ì¤‘ë³µ ì¤„ ì œê±°
            lines = full_text.split('\n')
            cleaned_lines = []
            prev_line = ""
            for line in lines:
                line = line.strip()
                if line and line != prev_line:  # ë¹ˆ ì¤„ê³¼ ì¤‘ë³µ ì œê±°
                    cleaned_lines.append(line)
                    prev_line = line
            
            full_text = '\n'.join(cleaned_lines)
            
            # ê¸¸ì´ ì œí•œ
            if len(full_text) > 8000:
                full_text = full_text[:8000] + "\n...(ë‚´ìš© ìƒëµ)"
            
            print(f"  [INFO] ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(full_text)}ì")
            
            return (url, full_text)
            
        except Exception as e:
            print(f"  [ERROR] ìƒì„¸ í˜ì´ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return (url, "")
    
    async def scrape_all_companies(self) -> List[Tuple[str, str]]:
        """
        ëª¨ë“  ì¹´ë“œì‚¬ ì´ë²¤íŠ¸ ìˆ˜ì§‘
        
        Returns:
            list: [(url, text), ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸
        """
        all_events = []
        
        await self.init_browser(headless=True)
        
        try:
            for company, list_url in self.card_companies.items():
                print(f"\n{'='*60}")
                print(f"ğŸ¢ {company} ìˆ˜ì§‘ ì‹œì‘")
                print(f"{'='*60}")
                
                # 1ë‹¨ê³„: ì´ë²¤íŠ¸ ëª©ë¡ URL ìˆ˜ì§‘
                event_urls = await self.extract_event_list_urls(company, list_url)
                
                # 2ë‹¨ê³„: ê° ì´ë²¤íŠ¸ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
                for idx, event_url in enumerate(event_urls, 1):
                    print(f"[{idx}/{len(event_urls)}] ", end="")
                    url, text = await self.extract_event_detail(event_url)
                    
                    if text:
                        all_events.append((url, text))
                    
                    # ë´‡ íƒì§€ ë°©ì§€ë¥¼ ìœ„í•œ ëœë¤ ë”œë ˆì´
                    await self.random_delay(2, 5)
                
                print(f"âœ… [{company}] ìˆ˜ì§‘ ì™„ë£Œ: {len(event_urls)}ê°œ\n")
        
        finally:
            await self.close_browser()
        
        print(f"\n{'='*60}")
        print(f"ğŸ‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_events)}ê°œ ì´ë²¤íŠ¸")
        print(f"{'='*60}")
        
        return all_events


async def test_scraper():
    """í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜"""
    scraper = CardEventScraper()
    
    # í…ŒìŠ¤íŠ¸: ì‹ í•œì¹´ë“œë§Œ ìˆ˜ì§‘
    scraper.card_companies = {
        "ì‹ í•œì¹´ë“œ": "https://www.shinhancard.com/pconts/html/benefit/event/main.html"
    }
    
    events = await scraper.scrape_all_companies()
    
    print("\n=== ìˆ˜ì§‘ ê²°ê³¼ ìƒ˜í”Œ ===")
    for idx, (url, text) in enumerate(events[:3], 1):
        print(f"\n[{idx}] {url}")
        print(f"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}ì")
        print(f"ë‚´ìš©: {text[:200]}...\n")


if __name__ == "__main__":
    asyncio.run(test_scraper())
