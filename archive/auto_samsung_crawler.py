"""
ìë™ ì‚¼ì„±ì¹´ë“œ í¬ë¡¤ëŸ¬ (ì„ íƒ ì—†ì´ ë°”ë¡œ ì‹¤í–‰)
cms_id ìˆœì°¨ ì¦ê°€ë¡œ 2026ë…„ ëª¨ë“  ì´ë²¤íŠ¸ ìˆ˜ì§‘
"""

import asyncio
import sys
import io
from playwright.async_api import async_playwright
from playwright_stealth import stealth_async
from bs4 import BeautifulSoup
from database import SessionLocal, insert_event, init_db

if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# ì‚¬ì´íŠ¸ëª…/í—¤ë”ë¡œ ì“¸ ìˆ˜ ìˆëŠ” ë¬¸êµ¬ â†’ ì´ë²¤íŠ¸ ì œëª©ìœ¼ë¡œ ì“°ì§€ ì•ŠìŒ
_HEADER_LIKE = ('ì‚¼ì„±ì¹´ë“œ', 'ì‚¼ì„± ì¹´ë“œ', 'samsungcard', 'samsung card', 'Samsung', 'SAMSUNG', 'ë¡œê·¸ì¸', 'ë§ˆì´í˜ì´ì§€', 'ì´ë²¤íŠ¸ ëª©ë¡', 'ê°œì¸ì¹´ë“œ', 'ê¸°ì—…ì¹´ë“œ')
# ì•Œë¦¼/ë°°ë„ˆ ë¬¸êµ¬ â†’ ì‹¤ì œ ì´ë²¤íŠ¸ ì œëª©ì´ ì•„ë‹˜ (ì œì™¸)
_NOTIFICATION_PREFIXES = ('ì´ë²¤íŠ¸ì— ì‘ëª¨ë˜ì—ˆìŠµë‹ˆë‹¤', 'ì´ë²¤íŠ¸ì— ì‘ëª¨ ë˜ì—ˆìŠµë‹ˆë‹¤', 'ì´ë²¤íŠ¸ì— ì‘ëª¨ëìŠµë‹ˆë‹¤', 'ì´ë²¤íŠ¸ì— ì‘ëª¨ ëìŠµë‹ˆë‹¤')


def _is_header_like(text):
    if not text or len(text) <= 5:
        return True
    t = text.strip()
    for h in _HEADER_LIKE:
        if t == h or t.startswith(h + ' ') or t.endswith(' ' + h):
            return True
    return False


def _is_notification_banner(text):
    """'ì´ë²¤íŠ¸ì— ì‘ëª¨ë˜ì—ˆìŠµë‹ˆë‹¤...' ê°™ì€ ì•Œë¦¼ ë¬¸êµ¬ë©´ True â†’ ì œëª© í›„ë³´ì—ì„œ ì œì™¸."""
    if not text or len(text) < 10:
        return False
    t = text.strip()
    if any(t.startswith(p) for p in _NOTIFICATION_PREFIXES):
        return True
    if 'ë§ˆì´í™ˆ ì•±ì˜' in t and 'ìì‚° ì—°ê²°' in t:
        return True
    return False


def _extract_event_title(soup, html):
    """ë³¸ë¬¸ ì´ë²¤íŠ¸ ì œëª©ë§Œ ì¶”ì¶œ (í—¤ë”/ì•Œë¦¼ ë¬¸êµ¬ ì œì™¸). 'ì´ë²¤íŠ¸ì— ì‘ëª¨ë˜ì—ˆìŠµë‹ˆë‹¤' ë“±ì€ ì œì™¸."""
    candidates = []

    def _ok(t):
        if not t or len(t) < 4:
            return False
        if _is_header_like(t) or _is_notification_banner(t):
            return False
        return True

    # 1) ë³¸ë¬¸ ì˜ì—­ ìš°ì„ 
    for scope_sel in ['main', '.content', '.event-detail', '[class*="event"]', '[class*="detail"]', '[class*="campaign"]', 'article']:
        scope = soup.select_one(scope_sel)
        if not scope:
            continue
        for tag in scope.find_all(['h1', 'h2', 'h3']):
            t = tag.get_text(strip=True)
            if _ok(t):
                candidates.append((len(t), t))
    # 2) ì „ì²´ h1, h2, h3 (í—¤ë”/ì•Œë¦¼ ì œì™¸)
    for tag in soup.find_all(['h1', 'h2', 'h3']):
        t = tag.get_text(strip=True)
        if _ok(t):
            candidates.append((len(t), t))
    # 3) í´ë˜ìŠ¤
    for sel in ['.event-title', '.title', '.tit', '.campaign-title', '[class*="tit"]']:
        try:
            el = soup.select_one(sel)
            if el:
                t = el.get_text(strip=True)
                if _ok(t):
                    candidates.append((len(t), t))
        except Exception:
            pass
    # 4) og:title
    try:
        og = soup.find('meta', property='og:title')
        if og and og.get('content'):
            t = og['content'].strip()
            for suffix in (' | ì‚¼ì„±ì¹´ë“œ', ' | Samsung', '- ì‚¼ì„±ì¹´ë“œ', '- Samsung'):
                if t.endswith(suffix):
                    t = t[:-len(suffix)].strip()
                    break
            if _ok(t):
                candidates.append((len(t), t))
    except Exception:
        pass
    if not candidates:
        return None
    seen = set()
    unique = []
    for _, t in sorted(candidates, key=lambda x: (-len(x[1]), x[1])):
        if t in seen or _is_notification_banner(t):
            continue
        seen.add(t)
        unique.append(t)
    for t in unique:
        if any(k in t for k in ('í˜œíƒ', 'í• ì¸', 'ìºì‹œë°±', 'í”„ë¡œëª¨ì…˜', 'ì„œ.í”„.ë¼', '2ì›”', '3ì›”', 'í• ë¶€', 'ë“±ë¡ê¸ˆ', 'ìë™ì°¨')):
            return t
    return unique[0] if unique else None


async def crawl_samsung_all():
    """ì‚¼ì„±ì¹´ë“œ ì „ì²´ í¬ë¡¤ë§"""
    
    print("\n" + "="*70)
    print("ğŸš€ ì‚¼ì„±ì¹´ë“œ ìˆœì°¨ í¬ë¡¤ëŸ¬ (ìë™ ì‹¤í–‰)")
    print("   ë²”ìœ„: cms_id 3733000 ~ 3737000 (2026ë…„ 2ì›” ì¤‘ì‹¬)")
    print("="*70 + "\n")
    
    base_url = "https://www.samsungcard.com/personal/event/ing/UHPPBE1403M0.jsp?cms_id="
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            viewport={'width': 1920, 'height': 1080}
        )
        
        page = await context.new_page()
        await stealth_async(page)
        
        collected = []
        consecutive_fails = 0
        
        # ë²”ìœ„ ì„¤ì •
        start_id = 3733000
        end_id = 3737000
        
        for cms_id in range(start_id, end_id + 1):
            # 100ê°œë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
            if (cms_id - start_id) % 100 == 0:
                print(f"\n[ì§„í–‰] {cms_id} ~ {cms_id + 99} íƒìƒ‰ ì¤‘...")
                print(f"  í˜„ì¬ê¹Œì§€: {len(collected)}ê°œ ë°œê²¬")
            
            url = f"{base_url}{cms_id}"
            
            try:
                await page.goto(url, wait_until="domcontentloaded", timeout=10000)
                await asyncio.sleep(1)
                
                html = await page.content()
                
                # "ì¡°íšŒ ê²°ê³¼ ì—†ìŒ" ì²´í¬
                if "ì¡°íšŒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤" in html or ("ì—†ìŠµë‹ˆë‹¤" in html and len(html) < 50000):
                    consecutive_fails += 1
                    if consecutive_fails >= 100:
                        print(f"\n[ì¢…ë£Œ] ì—°ì† 100ë²ˆ ì‹¤íŒ¨, íƒìƒ‰ ì¢…ë£Œ")
                        break
                    continue
                
                # ìœ íš¨í•œ ì´ë²¤íŠ¸!
                consecutive_fails = 0
                soup = BeautifulSoup(html, 'lxml')
                
                # ì œëª© (ì‚¬ì´íŠ¸ í—¤ë” "ì‚¼ì„±ì¹´ë“œ" ë“± ì œì™¸, ë³¸ë¬¸ ì´ë²¤íŠ¸ ì œëª©ë§Œ)
                title = _extract_event_title(soup, html)
                
                if not title or len(title) < 3:
                    continue
                
                # ê¸°ê°„
                period = "ì •ë³´ ì—†ìŒ"
                for kw in ['ê¸°ê°„', 'ì´ë²¤íŠ¸ ê¸°ê°„']:
                    elem = soup.find(string=lambda t: t and kw in t)
                    if elem:
                        period = elem.parent.get_text(strip=True)[:50] if elem.parent else elem[:50]
                        break
                
                # í˜œíƒ
                benefit = []
                for elem in soup.find_all(string=lambda t: t and any(k in t for k in ['í˜œíƒ', 'í• ì¸', 'ìºì‹œë°±']))[:3]:
                    text = elem.parent.get_text(strip=True) if elem.parent else elem
                    if 10 < len(text) < 150:
                        benefit.append(text)
                
                benefit_value = ' / '.join(benefit[:2]) if benefit else "ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°"
                
                # ì¹´í…Œê³ ë¦¬
                category = "ìƒí™œ"
                if any(w in title for w in ['ì—¬í–‰', 'í˜¸í…”']): category = "ì—¬í–‰"
                elif any(w in title for w in ['ì‡¼í•‘', 'í• ì¸']): category = "ì‡¼í•‘"
                elif any(w in title for w in ['ì‹ì‚¬', 'ë‹¤ì´ë‹', 'ìŠ¤íƒ€ë²…ìŠ¤', 'ì¹´í˜']): category = "ì‹ìŒë£Œ"
                elif any(w in title for w in ['ìë™ì°¨', 'ë³´í—˜']): category = "êµí†µ"
                elif any(w in title for w in ['ì˜í™”', 'ê³µì—°']): category = "ë¬¸í™”"
                elif any(w in title for w in ['ê¸ˆë¦¬', 'ëŒ€ì¶œ', 'í• ë¶€']): category = "ê¸ˆìœµ"
                
                # ìœ„í˜‘ë„
                threat = "Low"
                if any(w in title + benefit_value for w in ['10ë§Œì›', '20ë§Œì›', '30ë§Œì›', 'ìµœëŒ€']):
                    threat = "High"
                elif any(w in title + benefit_value for w in ['1ë§Œì›', '2ë§Œì›', '3ë§Œì›']):
                    threat = "Mid"
                
                event_data = {
                    "url": url,
                    "company": "ì‚¼ì„±ì¹´ë“œ",
                    "category": category,
                    "title": title,
                    "period": period,
                    "benefit_type": "í• ì¸" if "í• ì¸" in benefit_value else "ìºì‹œë°±" if "ìºì‹œë°±" in benefit_value else "ê¸°íƒ€",
                    "benefit_value": benefit_value,
                    "conditions": "ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°",
                    "target_segment": "ì¼ë°˜",
                    "threat_level": threat,
                    "one_line_summary": title,
                    "raw_text": soup.get_text(strip=True)[:1000]
                }
                
                collected.append(event_data)
                print(f"  âœ… [{cms_id}] {title[:45]}")
            
            except Exception as e:
                consecutive_fails += 1
                if consecutive_fails >= 100:
                    break
        
        await browser.close()
        
        print(f"\n{'='*70}")
        print(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ: {len(collected)}ê°œ ì´ë²¤íŠ¸")
        print(f"{'='*70}\n")
        
        return collected


def save_to_db(events):
    """DB ì €ì¥"""
    if not events:
        return
    
    print(f"[DB ì €ì¥] {len(events)}ê°œ ì €ì¥ ì¤‘...\n")
    
    init_db()
    db = SessionLocal()
    
    try:
        saved = 0
        duplicate = 0
        
        for event in events:
            success = insert_event(db, event)
            if success:
                saved += 1
                print(f"  âœ… {event['title'][:50]}")
            else:
                duplicate += 1
        
        print(f"\nì‹ ê·œ: {saved}ê°œ | ì¤‘ë³µ: {duplicate}ê°œ\n")
    
    finally:
        db.close()


async def main():
    """ë©”ì¸"""
    events = await crawl_samsung_all()
    save_to_db(events)
    print("\nâœ… ì™„ë£Œ! http://localhost:8000\n")


if __name__ == "__main__":
    asyncio.run(main())
