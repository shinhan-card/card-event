"""
Pure API Crawler - Gemini ì œê±°, í¬ë¡¤ë§ì—ë§Œ ì§‘ì¤‘
API ì¸í„°ì…‰íŠ¸ â†’ ì§ì ‘ íŒŒì‹± â†’ DB ì €ì¥

ëª©í‘œ: 2026ë…„ 4ê°œ ì¹´ë“œì‚¬ ëª¨ë“  ì´ë²¤íŠ¸ ìˆ˜ì§‘ (ì¢…ë£Œëœ ê²ƒ í¬í•¨)
"""

import asyncio
import sys
import io
import json
import os
import random
from datetime import datetime
from typing import List, Dict
from playwright.async_api import async_playwright, Page, Browser, Response
from playwright_stealth import stealth_async
from dotenv import load_dotenv
from database import SessionLocal, insert_event, init_db

if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

load_dotenv()


class PureAPICrawler:
    """ìˆœìˆ˜ API í¬ë¡¤ëŸ¬ (LLM ì œê±°)"""
    
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    ]
    
    CARD_COMPANIES = {
        "ì‹ í•œì¹´ë“œ": {
            "url": "https://www.shinhancard.com/pconts/html/benefit/event/main.html",
            "domain": "shinhancard.com",
            "api_parser": "parse_shinhan_api"
        },
        "ì‚¼ì„±ì¹´ë“œ": {
            "url": "https://www.samsungcard.com/personal/benefit/event/list.do",
            "domain": "samsungcard.com",
            "api_parser": "parse_samsung_api"
        },
        "í˜„ëŒ€ì¹´ë“œ": {
            "url": "https://www.hyundaicard.com/event/eventlist.hdc",
            "domain": "hyundaicard.com",
            "api_parser": "parse_hyundai_api"
        },
        "KBêµ­ë¯¼ì¹´ë“œ": {
            "url": "https://www.kbcard.com/CRD/DVIEW/MBCXBDDAMBC0001.do",
            "domain": "kbcard.com",
            "api_parser": "parse_kb_api"
        }
    }
    
    def __init__(self):
        self.browser: Browser = None
        self.page: Page = None
        self.intercepted_apis: List[Dict] = []
    
    async def init_browser(self, headless: bool = True):
        """Stealth ë¸Œë¼ìš°ì €"""
        playwright = await async_playwright().start()
        
        self.browser = await playwright.chromium.launch(
            headless=headless,
            args=['--disable-blink-features=AutomationControlled']
        )
        
        context = await self.browser.new_context(
            user_agent=random.choice(self.USER_AGENTS),
            viewport={'width': 1920, 'height': 1080},
            locale='ko-KR',
        )
        
        await context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            window.chrome = {runtime: {}};
        """)
        
        self.page = await context.new_page()
        await stealth_async(self.page)
        
        print("[OK] í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ\n")
    
    async def setup_api_interceptor(self, company_name: str, domain: str):
        """API ì¸í„°ì…‰í„° ì„¤ì •"""
        print(f"[{company_name}] API ëª¨ë‹ˆí„°ë§ ì‹œì‘...")
        
        async def handle_response(response: Response):
            try:
                url = response.url
                
                # ì¹´ë“œì‚¬ ë„ë©”ì¸ë§Œ
                if domain not in url:
                    return
                
                # JSONë§Œ
                content_type = response.headers.get('content-type', '')
                if 'json' not in content_type:
                    return
                
                # ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œ
                keywords = ['event', 'list', 'benefit', 'data', 'info']
                if not any(kw in url.lower() for kw in keywords):
                    return
                
                # ì œì™¸ íŒ¨í„´
                if any(ex in url.lower() for ex in ['tracking', 'analytics', 'mpulse', 'log']):
                    return
                
                try:
                    json_data = await response.json()
                    data_size = len(json.dumps(json_data))
                    
                    if data_size < 100:
                        return
                    
                    # ì´ë²¤íŠ¸ ê´€ë ¨ í‚¤ í™•ì¸
                    json_str = json.dumps(json_data, ensure_ascii=False).lower()
                    if any(ind in json_str for ind in ['title', 'ì œëª©', 'event', 'ì´ë²¤íŠ¸', 'name']):
                        
                        self.intercepted_apis.append({
                            'company': company_name,
                            'url': url,
                            'data': json_data,
                        })
                        
                        print(f"  âœ… API ìº¡ì²˜! [{len(self.intercepted_apis)}] {url[:60]}...")
                        print(f"     í¬ê¸°: {data_size} bytes")
                
                except:
                    pass
            except:
                pass
        
        self.page.on('response', handle_response)
    
    async def auto_scroll_and_load(self, max_scrolls: int = 30):
        """ìë™ ìŠ¤í¬ë¡¤ & ë”ë³´ê¸° (ê°•í™”íŒ)"""
        print(f"  ê°•í™” ìŠ¤í¬ë¡¤ ì‹œì‘ (ìµœëŒ€ {max_scrolls}íšŒ)...")
        
        for i in range(max_scrolls):
            # ìŠ¤í¬ë¡¤
            await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(2)  # ë” ê¸¸ê²Œ ëŒ€ê¸°
            
            # ë”ë³´ê¸° ë²„íŠ¼ ì°¾ê¸° (ë” ë§ì€ íŒ¨í„´)
            more_selectors = [
                'button:has-text("ë”ë³´ê¸°")',
                'button:has-text("ë” ë³´ê¸°")',
                'button:has-text("MORE")',
                'button:has-text("more")',
                'a:has-text("ë”ë³´ê¸°")',
                'a:has-text("ë” ë³´ê¸°")',
                '.more-btn', '.btn-more', '.load-more',
                'button.more', 'a.more',
                '[onclick*="more"]', '[onclick*="More"]',
            ]
            
            clicked = False
            for selector in more_selectors:
                try:
                    btn = await self.page.query_selector(selector)
                    if btn and await btn.is_visible():
                        print(f"  âœ… 'ë”ë³´ê¸°' í´ë¦­! (íšŒì°¨: {i+1})")
                        await btn.click()
                        await asyncio.sleep(3)  # í´ë¦­ í›„ ë” ê¸¸ê²Œ ëŒ€ê¸°
                        clicked = True
                        break
                except:
                    pass
            
            # ë²„íŠ¼ í´ë¦­í–ˆìœ¼ë©´ ë‹¤ì‹œ ìŠ¤í¬ë¡¤
            if clicked:
                await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(2)
        
        print(f"  ìŠ¤í¬ë¡¤ ì™„ë£Œ ({max_scrolls}íšŒ ì‹œë„)\n")
    
    # ==================== API íŒŒì„œë“¤ ====================
    
    def parse_samsung_api(self, api_data: Dict) -> List[Dict]:
        """ì‚¼ì„±ì¹´ë“œ API íŒŒì‹±"""
        events = []
        event_list = api_data.get('listPeiHPPPrgEvnInqrDVO', [])
        
        for event in event_list:
            title = event.get('cmpTitNm', '').strip()
            if not title or title == ' ':
                continue
            
            start = event.get('cmsCmpStrtdt', '')
            end = event.get('cmsCmpEnddt', '')
            event_id = event.get('cmpId', '')
            
            # ë‚ ì§œ í¬ë§·íŒ…
            start_fmt = f"{start[:4]}.{start[4:6]}.{start[6:8]}" if start and len(start) == 8 else ""
            end_fmt = f"{end[:4]}.{end[4:6]}.{end[6:8]}" if end and len(end) == 8 else ""
            period = f"{start_fmt}~{end_fmt}" if start_fmt and end_fmt else "ì •ë³´ ì—†ìŒ"
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ë¡ 
            category = self.infer_category(title)
            threat_level = self.infer_threat(title)
            
            # ì˜¬ë°”ë¥¸ URL ìƒì„± (cmsId ì‚¬ìš©)
            cms_id = event.get('cmsId', '')
            if cms_id:
                url = f"https://www.samsungcard.com/personal/event/ing/UHPPBE1403M0.jsp?cms_id={cms_id}"
            else:
                url = f"https://www.samsungcard.com/personal/benefit/event/view.do?evtId={event_id}"
            
            events.append({
                "url": url,
                "company": "ì‚¼ì„±ì¹´ë“œ",
                "category": category,
                "title": title,
                "period": period,
                "benefit_type": "ì •ë³´ ì—†ìŒ",
                "benefit_value": "ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°",
                "conditions": "ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°",
                "target_segment": "ì¼ë°˜",
                "threat_level": threat_level,
                "one_line_summary": title,
                "raw_text": json.dumps(event, ensure_ascii=False)[:500]
            })
        
        return events
    
    def parse_shinhan_api(self, api_data: Dict) -> List[Dict]:
        """ì‹ í•œì¹´ë“œ API íŒŒì‹± (êµ¬ì¡° íŒŒì•… í›„ êµ¬í˜„)"""
        # TODO: ì‹ í•œì¹´ë“œ API êµ¬ì¡°ì— ë§ê²Œ íŒŒì‹±
        return []
    
    def parse_hyundai_api(self, api_data: Dict) -> List[Dict]:
        """í˜„ëŒ€ì¹´ë“œ API íŒŒì‹± (êµ¬ì¡° íŒŒì•… í›„ êµ¬í˜„)"""
        # TODO: í˜„ëŒ€ì¹´ë“œ API êµ¬ì¡°ì— ë§ê²Œ íŒŒì‹±
        return []
    
    def parse_kb_api(self, api_data: Dict) -> List[Dict]:
        """KBêµ­ë¯¼ì¹´ë“œ API íŒŒì‹± (êµ¬ì¡° íŒŒì•… í›„ êµ¬í˜„)"""
        # TODO: KBêµ­ë¯¼ì¹´ë“œ API êµ¬ì¡°ì— ë§ê²Œ íŒŒì‹±
        return []
    
    def infer_category(self, title: str) -> str:
        """ì œëª©ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ì¶”ë¡ """
        if any(w in title for w in ['ì—¬í–‰', 'í˜¸í…”', 'í•­ê³µ', 'ë¦¬ì¡°íŠ¸']):
            return "ì—¬í–‰"
        elif any(w in title for w in ['ì‡¼í•‘', 'í• ì¸', 'ë°±í™”ì ', 'ë§ˆíŠ¸']):
            return "ì‡¼í•‘"
        elif any(w in title for w in ['ì‹ì‚¬', 'ë ˆìŠ¤í† ë‘', 'ë‹¤ì´ë‹', 'ìŠ¤íƒ€ë²…ìŠ¤', 'ì¹´í˜']):
            return "ì‹ìŒë£Œ"
        elif any(w in title for w in ['ìë™ì°¨', 'ë³´í—˜', 'ì£¼ìœ ', 'ì°¨ëŸ‰']):
            return "êµí†µ"
        elif any(w in title for w in ['ì˜í™”', 'ê³µì—°', 'ë¬¸í™”', 'CGV']):
            return "ë¬¸í™”"
        elif any(w in title for w in ['ê¸ˆë¦¬', 'ëŒ€ì¶œ', 'í• ë¶€', 'ê¸ˆìœµ']):
            return "ê¸ˆìœµ"
        elif any(w in title for w in ['í†µì‹ ', 'ë„·í”Œë¦­ìŠ¤', 'ìœ íŠœë¸Œ', 'êµ¬ë…']):
            return "í†µì‹ "
        else:
            return "ê¸°íƒ€"
    
    def infer_threat(self, title: str) -> str:
        """ìœ„í˜‘ë„ ì¶”ë¡ """
        if any(w in title for w in ['10ë§Œì›', '20ë§Œì›', '30ë§Œì›', 'ìµœëŒ€', 'í”„ë¦¬ë¯¸ì—„']):
            return "High"
        elif any(w in title for w in ['1ë§Œì›', '2ë§Œì›', '3ë§Œì›', '5ì²œì›']):
            return "Mid"
        else:
            return "Low"
    
    async def crawl_company(self, company: str, config: Dict) -> List[Dict]:
        """ë‹¨ì¼ ì¹´ë“œì‚¬ í¬ë¡¤ë§"""
        print("="*70)
        print(f"[{company}] í¬ë¡¤ë§ ì‹œì‘")
        print("="*70 + "\n")
        
        self.intercepted_apis = []
        await self.setup_api_interceptor(company, config['domain'])
        
        try:
            # í˜ì´ì§€ ë¡œë”©
            print(f"  í˜ì´ì§€ ë¡œë”©: {config['url'][:60]}...")
            await self.page.goto(config['url'], timeout=60000)
            await asyncio.sleep(3)
            
            # ìë™ ìŠ¤í¬ë¡¤ & ë”ë³´ê¸°
            await self.auto_scroll_and_load(max_scrolls=15)
            
            # ì¶”ê°€ ëŒ€ê¸° (API ì™„ë£Œ)
            await asyncio.sleep(3)
            
            print(f"\n[ê²°ê³¼] API ìº¡ì²˜: {len(self.intercepted_apis)}ê°œ\n")
            
            # API ë°ì´í„° íŒŒì‹±
            all_events = []
            
            if self.intercepted_apis:
                parser_name = config.get('api_parser')
                parser_method = getattr(self, parser_name, None)
                
                for i, api_item in enumerate(self.intercepted_apis, 1):
                    print(f"  [{i}/{len(self.intercepted_apis)}] API íŒŒì‹± ì¤‘...")
                    
                    # íŒŒì¼ ì €ì¥
                    filename = f"api_{company}_{i}.json"
                    with open(filename, 'w', encoding='utf-8') as f:
                        json.dump(api_item['data'], f, ensure_ascii=False, indent=2)
                    print(f"      ì €ì¥: {filename}")
                    
                    if parser_method:
                        parsed_events = parser_method(api_item['data'])
                        all_events.extend(parsed_events)
                        print(f"      íŒŒì‹±: {len(parsed_events)}ê°œ ì´ë²¤íŠ¸")
            
            print(f"\n[{company}] ìˆ˜ì§‘ ì™„ë£Œ: {len(all_events)}ê°œ\n")
            return all_events
            
        except Exception as e:
            print(f"[ERROR] {company}: {e}\n")
            return []
    
    async def crawl_all(self) -> List[Dict]:
        """ì „ì²´ í¬ë¡¤ë§"""
        print("\n" + "="*70)
        print("ğŸš€ Pure API Crawler ì‹œì‘ (Gemini ì œê±°, í¬ë¡¤ë§ ì§‘ì¤‘)")
        print(f"   {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*70 + "\n")
        
        all_events = []
        await self.init_browser(headless=True)
        
        try:
            for company, config in self.CARD_COMPANIES.items():
                events = await self.crawl_company(company, config)
                all_events.extend(events)
                await asyncio.sleep(3)
            
            print("="*70)
            print(f"ğŸ‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {len(all_events)}ê°œ")
            print("="*70 + "\n")
            
            # ì¹´ë“œì‚¬ë³„ í†µê³„
            stats = {}
            for e in all_events:
                comp = e.get('company', 'ì•Œìˆ˜ì—†ìŒ')
                stats[comp] = stats.get(comp, 0) + 1
            
            for comp, count in stats.items():
                print(f"  - {comp}: {count}ê°œ")
            
            print()
            return all_events
            
        finally:
            if self.browser:
                await self.browser.close()
    
    def save_to_db(self, events: List[Dict]):
        """DB ì €ì¥"""
        if not events:
            print("[WARN] ì €ì¥í•  ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\n")
            return
        
        print("="*70)
        print(f"[DB ì €ì¥] {len(events)}ê°œ ì´ë²¤íŠ¸ ì €ì¥ ì¤‘...")
        print("="*70 + "\n")
        
        init_db()
        db = SessionLocal()
        
        try:
            saved = 0
            duplicate = 0
            
            for i, event in enumerate(events, 1):
                print(f"  [{i:2d}/{len(events)}] {event['title'][:50]}")
                
                success = insert_event(db, event)
                if success:
                    saved += 1
                    print(f"       âœ… ì €ì¥")
                else:
                    duplicate += 1
                    print(f"       âš ï¸ ì¤‘ë³µ")
            
            print(f"\n{'='*70}")
            print(f"[ì™„ë£Œ]")
            print(f"  ì‹ ê·œ ì €ì¥: {saved}ê°œ")
            print(f"  ì¤‘ë³µ ìŠ¤í‚µ: {duplicate}ê°œ")
            print(f"{'='*70}\n")
        
        finally:
            db.close()


async def main():
    """ë©”ì¸ ì‹¤í–‰"""
    crawler = PureAPICrawler()
    
    # í¬ë¡¤ë§
    events = await crawler.crawl_all()
    
    # DB ì €ì¥
    crawler.save_to_db(events)
    
    print("\n[ì™„ë£Œ] ëŒ€ì‹œë³´ë“œ: http://localhost:8000\n")


if __name__ == "__main__":
    asyncio.run(main())
