"""
ì‚¼ì„±ì¹´ë“œ ìƒì„¸ íŽ˜ì´ì§€ í¬ë¡¤ëŸ¬
ëª©ë¡ì—ì„œ ê° ì´ë²¤íŠ¸ URLì„ ì¶”ì¶œí•˜ì—¬ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
"""

import asyncio
import sys
import io
import json
from playwright.async_api import async_playwright
from playwright_stealth import stealth_async
from bs4 import BeautifulSoup
from database import SessionLocal, get_all_events, CardEvent
from sqlalchemy import update

if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


class SamsungDetailCrawler:
    """ì‚¼ì„±ì¹´ë“œ ìƒì„¸ í¬ë¡¤ëŸ¬"""
    
    async def crawl_detail_page(self, page, url: str) -> dict:
        """ë‹¨ì¼ ìƒì„¸ íŽ˜ì´ì§€ í¬ë¡¤ë§ (íŽ˜ì´ì§€ ìž¬ì‚¬ìš©)"""
        try:
            print(f"  í¬ë¡¤ë§ ì¤‘: {url[:60]}...")
            
            # íŽ˜ì´ì§€ ë¡œë”© (wait_until ì˜µì…˜ ì œê±°ë¡œ ë” ì•ˆì •ì )
            await page.goto(url, wait_until="domcontentloaded", timeout=30000)
            await asyncio.sleep(5)  # ë” ê¸´ ëŒ€ê¸°
                
            # ìŠ¤í¬ë¡¤í•˜ì—¬ ì „ì²´ ì½˜í…ì¸  ë¡œë“œ
            try:
                for _ in range(3):
                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    await asyncio.sleep(1)
            except:
                pass  # ìŠ¤í¬ë¡¤ ì˜¤ë¥˜ ë¬´ì‹œ
                
            html = await page.content()
            soup = BeautifulSoup(html, 'lxml')
                
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                tag.decompose()
            
            # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            detail_info = {}
                
            # 1. í˜œíƒ ë‚´ìš© ì¶”ì¶œ
            benefit_keywords = ['í˜œíƒ', 'í• ì¸', 'ì ë¦½', 'ìºì‹œë°±', 'í¬ì¸íŠ¸', 'ë¬´ë£Œ', 'ì¦ì •']
            benefit_sections = soup.find_all(['div', 'section', 'p', 'li', 'td'], 
                string=lambda text: text and any(kw in text for kw in benefit_keywords))
                
            benefits = []
            for section in benefit_sections[:10]:
                text = section.get_text(strip=True)
                if len(text) > 10 and len(text) < 200:
                    benefits.append(text)
            
            detail_info['benefit_value'] = ' / '.join(benefits[:3]) if benefits else "ìƒì„¸ íŽ˜ì´ì§€ ì°¸ì¡°"
                
            # 2. ì°¸ì—¬ ì¡°ê±´ ì¶”ì¶œ
            condition_keywords = ['ì¡°ê±´', 'ëŒ€ìƒ', 'ì œì™¸', 'ìœ ì˜ì‚¬í•­', 'ì°¸ì—¬ë°©ë²•']
            condition_sections = soup.find_all(['div', 'section', 'p', 'li', 'td'],
                string=lambda text: text and any(kw in text for kw in condition_keywords))
            
            conditions = []
            for section in condition_sections[:10]:
                text = section.get_text(strip=True)
                if len(text) > 10 and len(text) < 200:
                    conditions.append(text)
            
            detail_info['conditions'] = ' / '.join(conditions[:3]) if conditions else "ìƒì„¸ íŽ˜ì´ì§€ ì°¸ì¡°"
            
            # 3. ëŒ€ìƒ ì¹´ë“œ ì¶”ì¶œ
            card_keywords = ['ëŒ€ìƒì¹´ë“œ', 'í•´ë‹¹ì¹´ë“œ', 'ì ìš©ì¹´ë“œ']
            target_card = soup.find(['div', 'section', 'p', 'span'],
                string=lambda text: text and any(kw in text for kw in card_keywords))
            
            if target_card:
                detail_info['target_segment'] = target_card.get_text(strip=True)[:100]
            else:
                detail_info['target_segment'] = "ì „ì²´ì¹´ë“œ"
            
            # 4. í˜œíƒ ìœ í˜• ì¶”ë¡ 
            full_text = soup.get_text().lower()
            if 'í• ì¸' in full_text:
                detail_info['benefit_type'] = "í• ì¸"
            elif 'ìºì‹œë°±' in full_text:
                detail_info['benefit_type'] = "ìºì‹œë°±"
            elif 'í¬ì¸íŠ¸' in full_text or 'ì ë¦½' in full_text:
                detail_info['benefit_type'] = "í¬ì¸íŠ¸ì ë¦½"
            elif 'ë¬´ì´ìž' in full_text or 'í• ë¶€' in full_text:
                detail_info['benefit_type'] = "ë¬´ì´ìží• ë¶€"
            elif 'ì¦ì •' in full_text or 'ë¬´ë£Œ' in full_text:
                detail_info['benefit_type'] = "ì‚¬ì€í’ˆ"
            else:
                detail_info['benefit_type'] = "ê¸°íƒ€"
            
            # 5. ì›ë³¸ í…ìŠ¤íŠ¸ ì €ìž¥ (ì „ì²´ ë‚´ìš©)
            body_text = soup.get_text(separator='\n', strip=True)
            lines = [line for line in body_text.split('\n') if line.strip()]
            detail_info['raw_text'] = '\n'.join(lines)[:2000]  # 2000ìžê¹Œì§€
            
            print(f"    âœ… ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")
            print(f"       í˜œíƒ: {detail_info['benefit_value'][:50]}...")
            print(f"       ì¡°ê±´: {detail_info['conditions'][:50]}...")
            
            return detail_info
        
        except Exception as e:
            print(f"    âŒ ì˜¤ë¥˜: {e}")
            return {}
    
    async def crawl_all_details(self):
        """DBì˜ ëª¨ë“  ì‚¼ì„±ì¹´ë“œ ì´ë²¤íŠ¸ì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§"""
        print("\n" + "="*70)
        print("ðŸš€ ì‚¼ì„±ì¹´ë“œ ìƒì„¸ íŽ˜ì´ì§€ í¬ë¡¤ëŸ¬ (ê°œì„ íŒ)")
        print("="*70 + "\n")
        
        # ë¸Œë¼ìš°ì € ì´ˆê¸°í™” (í•œ ë²ˆë§Œ)
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
                viewport={'width': 1920, 'height': 1080}
            )
            
            page = await context.new_page()
            await stealth_async(page)
            
            # DBì—ì„œ ì‚¼ì„±ì¹´ë“œ ì´ë²¤íŠ¸ ì¡°íšŒ
            db = SessionLocal()
            
            try:
                samsung_events = db.query(CardEvent).filter(
                    CardEvent.company == "ì‚¼ì„±ì¹´ë“œ"
                ).all()
                
                print(f"[INFO] ì‚¼ì„±ì¹´ë“œ ì´ë²¤íŠ¸: {len(samsung_events)}ê°œ\n")
                
                if not samsung_events:
                    print("[WARN] ì‚¼ì„±ì¹´ë“œ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return
                
                success_count = 0
                
                for i, event in enumerate(samsung_events, 1):
                    print(f"\n[{i}/{len(samsung_events)}] {event.title[:50]}")
                    
                    # ì´ë¯¸ ìƒì„¸ ì •ë³´ê°€ ìžˆìœ¼ë©´ ìŠ¤í‚µ
                    if event.benefit_value and event.benefit_value != "ì •ë³´ ì—†ìŒ" and event.benefit_value != "ìƒì„¸ íŽ˜ì´ì§€ ì°¸ì¡°":
                        print(f"  â­ï¸  ì´ë¯¸ ìƒì„¸ ì •ë³´ ìžˆìŒ, ìŠ¤í‚µ")
                        continue
                    
                    # ìƒì„¸ íŽ˜ì´ì§€ í¬ë¡¤ë§ (page ê°ì²´ ìž¬ì‚¬ìš©)
                    detail_info = await self.crawl_detail_page(page, event.url)
                    
                    if detail_info:
                        # DB ì—…ë°ì´íŠ¸
                        event.benefit_type = detail_info.get('benefit_type', event.benefit_type)
                        event.benefit_value = detail_info.get('benefit_value', event.benefit_value)
                        event.conditions = detail_info.get('conditions', event.conditions)
                        event.target_segment = detail_info.get('target_segment', event.target_segment)
                        event.raw_text = detail_info.get('raw_text', event.raw_text)
                        
                        db.commit()
                        success_count += 1
                        print(f"    ðŸ’¾ DB ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                    
                    # ë‹¤ìŒ íŽ˜ì´ì§€ ì „ ëŒ€ê¸°
                    if i < len(samsung_events):
                        await asyncio.sleep(3)
                
                print(f"\n{'='*70}")
                print(f"[ì™„ë£Œ] {success_count}/{len(samsung_events)}ê°œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘")
                print(f"{'='*70}\n")
                
                print("[INFO] ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸: http://localhost:8000\n")
            
            finally:
                db.close()
                await browser.close()


async def main():
    """ë©”ì¸ ì‹¤í–‰"""
    crawler = SamsungDetailCrawler()
    await crawler.crawl_all_details()


if __name__ == "__main__":
    asyncio.run(main())
