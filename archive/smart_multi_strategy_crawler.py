"""
Smart Multi-Strategy Crawler
ê° ì¹´ë“œì‚¬ë³„ë¡œ ìµœì ì˜ í¬ë¡¤ë§ ì „ëµ ì ìš©

ì „ëµ:
- ì‚¼ì„±ì¹´ë“œ: API ì¸í„°ì…‰íŠ¸ âœ… (ì´ë¯¸ ì„±ê³µ)
- ì‹ í•œì¹´ë“œ: í˜ì´ì§€ë„¤ì´ì…˜ + ì§ì ‘ ì¶”ì¶œ
- í˜„ëŒ€ì¹´ë“œ: ì¹´í…Œê³ ë¦¬ íƒ­ ìˆœíšŒ + API ì¸í„°ì…‰íŠ¸
- KBêµ­ë¯¼ì¹´ë“œ: iframe + ì§ì ‘ ì¶”ì¶œ
"""

import asyncio
import sys
import io
import json
import os
import random
from typing import List, Dict
from playwright.async_api import async_playwright, Page, Browser, Response
from playwright_stealth import stealth_async
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from database import SessionLocal, insert_event, init_db

if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

load_dotenv()


class SmartMultiStrategyCrawler:
    """ì¹´ë“œì‚¬ë³„ ë§ì¶¤í˜• í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.browser: Browser = None
        self.page: Page = None
        self.captured_apis: List[Dict] = []
    
    async def init_browser(self, headless: bool = True):
        """Stealth ë¸Œë¼ìš°ì €"""
        playwright = await async_playwright().start()
        
        self.browser = await playwright.chromium.launch(
            headless=headless,
            args=['--disable-blink-features=AutomationControlled', '--window-size=1920,1080']
        )
        
        context = await self.browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            viewport={'width': 1920, 'height': 1080},
            locale='ko-KR',
        )
        
        await context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            window.chrome = {runtime: {}};
        """)
        
        self.page = await context.new_page()
        await stealth_async(self.page)
        
        print("[OK] ë¸Œë¼ìš°ì € ì´ˆê¸°í™”\n")
    
    # ==================== ì‚¼ì„±ì¹´ë“œ: API ì¸í„°ì…‰íŠ¸ ====================
    
    async def crawl_samsung(self) -> List[Dict]:
        """ì‚¼ì„±ì¹´ë“œ: API ì¸í„°ì…‰íŠ¸ (ì´ë¯¸ ê²€ì¦ë¨)"""
        print("="*70)
        print("[ì‚¼ì„±ì¹´ë“œ] API ì¸í„°ì…‰íŠ¸ ì „ëµ")
        print("="*70 + "\n")
        
        self.captured_apis = []
        
        async def handle_response(response: Response):
            try:
                url = response.url
                if 'samsungcard.com' not in url:
                    return
                if 'json' not in response.headers.get('content-type', ''):
                    return
                if not any(k in url.lower() for k in ['event', 'list', 'benefit']):
                    return
                if any(e in url.lower() for e in ['tracking', 'mpulse']):
                    return
                
                json_data = await response.json()
                data_size = len(json.dumps(json_data))
                if data_size < 100:
                    return
                
                self.captured_apis.append({'data': json_data, 'url': url})
                print(f"  âœ… API ìº¡ì²˜! {url[:60]}... ({data_size} bytes)")
            except:
                pass
        
        self.page.on('response', handle_response)
        
        await self.page.goto("https://www.samsungcard.com/personal/benefit/event/list.do", timeout=60000)
        await asyncio.sleep(5)
        
        # ë”ë³´ê¸° í´ë¦­
        for i in range(10):
            try:
                btn = await self.page.query_selector('button:has-text("ë”ë³´ê¸°")')
                if btn and await btn.is_visible():
                    await btn.click()
                    print("  âœ… ë”ë³´ê¸° í´ë¦­")
                    await asyncio.sleep(3)
            except Exception as e:
                pass
            
            try:
                await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(2)
            except Exception as e:
                # í˜ì´ì§€ ì „í™˜ ì˜¤ë¥˜ ë¬´ì‹œí•˜ê³  ê³„ì†
                print(f"  [WARN] ìŠ¤í¬ë¡¤ ì˜¤ë¥˜ ë¬´ì‹œ")
                break
        
        # íŒŒì‹±
        events = []
        for api in self.captured_apis:
            event_list = api['data'].get('listPeiHPPPrgEvnInqrDVO', [])
            for event in event_list:
                title = event.get('cmpTitNm', '').strip()
                if not title or title == ' ':
                    continue
                
                events.append(self.parse_samsung_event(event))
        
        print(f"\n[ì‚¼ì„±ì¹´ë“œ] ìˆ˜ì§‘: {len(events)}ê°œ\n")
        return events
    
    def parse_samsung_event(self, event: Dict) -> Dict:
        """ì‚¼ì„±ì¹´ë“œ ì´ë²¤íŠ¸ íŒŒì‹±"""
        title = event.get('cmpTitNm', '').strip()
        start = event.get('cmsCmpStrtdt', '')
        end = event.get('cmsCmpEnddt', '')
        event_id = event.get('cmpId', '')
        
        start_fmt = f"{start[:4]}.{start[4:6]}.{start[6:8]}" if start and len(start) == 8 else ""
        end_fmt = f"{end[:4]}.{end[4:6]}.{end[6:8]}" if end and len(end) == 8 else ""
        period = f"{start_fmt}~{end_fmt}" if start_fmt else "ì •ë³´ ì—†ìŒ"
        
        # ì˜¬ë°”ë¥¸ URL ìƒì„± (cmsId ì‚¬ìš©)
        cms_id = event.get('cmsId', '')
        if cms_id:
            url = f"https://www.samsungcard.com/personal/event/ing/UHPPBE1403M0.jsp?cms_id={cms_id}"
        else:
            url = f"https://www.samsungcard.com/personal/benefit/event/view.do?evtId={event_id}"
        
        return {
            "url": url,
            "company": "ì‚¼ì„±ì¹´ë“œ",
            "category": self.infer_category(title),
            "title": title,
            "period": period,
            "benefit_type": "ì •ë³´ ì—†ìŒ",
            "benefit_value": "ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°",
            "conditions": "ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°",
            "target_segment": "ì¼ë°˜",
            "threat_level": self.infer_threat(title),
            "one_line_summary": title,
            "raw_text": json.dumps(event, ensure_ascii=False)[:500]
        }
    
    # ==================== ì‹ í•œì¹´ë“œ: HTML ì§ì ‘ íŒŒì‹± ====================
    
    async def crawl_shinhan(self) -> List[Dict]:
        """ì‹ í•œì¹´ë“œ: HTML ì§ì ‘ íŒŒì‹±"""
        print("="*70)
        print("[ì‹ í•œì¹´ë“œ] HTML íŒŒì‹± ì „ëµ")
        print("="*70 + "\n")
        
        await self.page.goto("https://www.shinhancard.com/pconts/html/benefit/event/main.html", timeout=60000)
        await asyncio.sleep(5)
        
        # ê°•ë ¥í•œ ìŠ¤í¬ë¡¤
        for i in range(20):
            await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(1)
        
        # HTML íŒŒì‹±
        html = await self.page.content()
        soup = BeautifulSoup(html, 'lxml')
        
        events = []
        
        # ì´ë²¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸° (ë‹¤ì–‘í•œ íŒ¨í„´)
        event_containers = soup.select('.event-list li, .list-item, article.event, div.event-card, .event-box')
        
        print(f"  ë°œê²¬ëœ ì»¨í…Œì´ë„ˆ: {len(event_containers)}ê°œ")
        
        for container in event_containers[:50]:
            # ì œëª© ì°¾ê¸°
            title_elem = container.select_one('h3, h4, .title, .event-title, strong')
            if not title_elem:
                continue
            
            title = title_elem.get_text(strip=True)
            if len(title) < 3:
                continue
            
            # ë§í¬ ì°¾ê¸°
            link_elem = container.find('a')
            if link_elem:
                href = link_elem.get('href', '')
                if href.startswith('/'):
                    url = f"https://www.shinhancard.com{href}"
                elif href.startswith('http'):
                    url = href
                else:
                    url = f"https://www.shinhancard.com/pconts/html/benefit/event/{href}"
            else:
                url = "https://www.shinhancard.com/pconts/html/benefit/event/main.html"
            
            # ê¸°ê°„ ì°¾ê¸°
            period_elem = container.select_one('.period, .date, .event-date, span.date')
            period = period_elem.get_text(strip=True) if period_elem else "ì •ë³´ ì—†ìŒ"
            
            events.append({
                "url": url,
                "company": "ì‹ í•œì¹´ë“œ",
                "category": self.infer_category(title),
                "title": title,
                "period": period,
                "benefit_type": "ì •ë³´ ì—†ìŒ",
                "benefit_value": "ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°",
                "conditions": "ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°",
                "target_segment": "ì¼ë°˜",
                "threat_level": self.infer_threat(title),
                "one_line_summary": title,
                "raw_text": container.get_text(strip=True)[:500]
            })
        
        print(f"\n[ì‹ í•œì¹´ë“œ] ìˆ˜ì§‘: {len(events)}ê°œ\n")
        return events
    
    # ==================== í˜„ëŒ€ì¹´ë“œ: íƒ­ ìˆœíšŒ + API ====================
    
    async def crawl_hyundai(self) -> List[Dict]:
        """í˜„ëŒ€ì¹´ë“œ: ì¹´í…Œê³ ë¦¬ íƒ­ í´ë¦­ + API ì¸í„°ì…‰íŠ¸"""
        print("="*70)
        print("[í˜„ëŒ€ì¹´ë“œ] íƒ­ ìˆœíšŒ + API ì „ëµ")
        print("="*70 + "\n")
        
        self.captured_apis = []
        
        async def handle_response(response: Response):
            try:
                url = response.url
                if 'hyundaicard.com' not in url:
                    return
                if 'json' not in response.headers.get('content-type', ''):
                    return
                if 'event' not in url.lower() and 'list' not in url.lower():
                    return
                
                json_data = await response.json()
                if len(json.dumps(json_data)) > 100:
                    self.captured_apis.append({'data': json_data, 'url': url})
                    print(f"  âœ… API ìº¡ì²˜! {url[:60]}...")
            except:
                pass
        
        self.page.on('response', handle_response)
        
        await self.page.goto("https://www.hyundaicard.com/event/eventlist.hdc", timeout=60000)
        await asyncio.sleep(5)
        
        # ëª¨ë“  íƒ­/ì¹´í…Œê³ ë¦¬ í´ë¦­ ì‹œë„
        tab_selectors = [
            'button', 'a.tab', '.tab-item', '.category', '[role="tab"]',
            'li.tab', 'button.category'
        ]
        
        for selector in tab_selectors:
            try:
                tabs = await self.page.query_selector_all(selector)
                for i, tab in enumerate(tabs[:10]):
                    try:
                        if await tab.is_visible():
                            await tab.click()
                            print(f"  íƒ­ í´ë¦­: {i+1}")
                            await asyncio.sleep(2)
                    except:
                        pass
            except:
                pass
        
        # ìŠ¤í¬ë¡¤
        for _ in range(30):
            await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(1)
        
        # APIê°€ ì—†ìœ¼ë©´ HTML íŒŒì‹±
        if not self.captured_apis:
            print("  API ì—†ìŒ, HTML íŒŒì‹±...")
            html = await self.page.content()
            soup = BeautifulSoup(html, 'lxml')
            
            events = []
            containers = soup.select('.event-item, .card-item, article, li.item')[:50]
            
            for container in containers:
                title_elem = container.select_one('h3, h4, .title, strong')
                if not title_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                if len(title) < 3:
                    continue
                
                link = container.find('a')
                url = link.get('href', '') if link else ""
                if url and url.startswith('/'):
                    url = f"https://www.hyundaicard.com{url}"
                
                events.append({
                    "url": url or "https://www.hyundaicard.com/event/eventlist.hdc",
                    "company": "í˜„ëŒ€ì¹´ë“œ",
                    "category": self.infer_category(title),
                    "title": title,
                    "period": "ì •ë³´ ì—†ìŒ",
                    "benefit_type": "ì •ë³´ ì—†ìŒ",
                    "benefit_value": "ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°",
                    "conditions": "ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°",
                    "target_segment": "ì¼ë°˜",
                    "threat_level": self.infer_threat(title),
                    "one_line_summary": title,
                    "raw_text": container.get_text(strip=True)[:500]
                })
            
            print(f"\n[í˜„ëŒ€ì¹´ë“œ] ìˆ˜ì§‘: {len(events)}ê°œ\n")
            return events
        
        # API íŒŒì‹± (TODO)
        print(f"\n[í˜„ëŒ€ì¹´ë“œ] API ìˆ˜ì§‘: {len(self.captured_apis)}ê°œ\n")
        return []
    
    # ==================== KBêµ­ë¯¼ì¹´ë“œ: Selenium ìŠ¤íƒ€ì¼ ëŒ€ê¸° ====================
    
    async def crawl_kb(self) -> List[Dict]:
        """KBêµ­ë¯¼ì¹´ë“œ: ê¸´ ëŒ€ê¸° + HTML íŒŒì‹±"""
        print("="*70)
        print("[KBêµ­ë¯¼ì¹´ë“œ] ê¸´ ëŒ€ê¸° + HTML ì „ëµ")
        print("="*70 + "\n")
        
        await self.page.goto("https://www.kbcard.com/CRD/DVIEW/MBCXBDDAMBC0001.do", timeout=60000)
        
        # ë§¤ìš° ê¸´ ëŒ€ê¸° (JavaScript ì™„ì „ ë¡œë”©)
        print("  í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (10ì´ˆ)...")
        await asyncio.sleep(10)
        
        # ëª¨ë“  ìš”ì†Œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        try:
            await self.page.wait_for_selector('body', timeout=10000)
        except:
            pass
        
        # ê°•ë ¥í•œ ìŠ¤í¬ë¡¤
        for _ in range(30):
            await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(1.5)
        
        # HTML íŒŒì‹±
        html = await self.page.content()
        soup = BeautifulSoup(html, 'lxml')
        
        events = []
        
        # KBêµ­ë¯¼ì¹´ë“œ íŠ¹í™” ì…€ë ‰í„°
        containers = soup.select('div.event-list li, tr, div.item, article')[:50]
        
        print(f"  ë°œê²¬ëœ ìš”ì†Œ: {len(containers)}ê°œ")
        
        for container in containers:
            title_elem = container.select_one('h3, h4, td.title, .title, strong, span.subject')
            if not title_elem:
                continue
            
            title = title_elem.get_text(strip=True)
            if len(title) < 3 or 'ì´ë²¤íŠ¸' not in title and 'í˜œíƒ' not in title:
                continue
            
            link = container.find('a')
            url = link.get('href', '') if link else ""
            if url and url.startswith('/'):
                url = f"https://www.kbcard.com{url}"
            
            events.append({
                "url": url or "https://www.kbcard.com/CRD/DVIEW/MBCXBDDAMBC0001.do",
                "company": "KBêµ­ë¯¼ì¹´ë“œ",
                "category": self.infer_category(title),
                "title": title,
                "period": "ì •ë³´ ì—†ìŒ",
                "benefit_type": "ì •ë³´ ì—†ìŒ",
                "benefit_value": "ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°",
                "conditions": "ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°",
                "target_segment": "ì¼ë°˜",
                "threat_level": self.infer_threat(title),
                "one_line_summary": title,
                "raw_text": container.get_text(strip=True)[:500]
            })
        
        print(f"\n[KBêµ­ë¯¼ì¹´ë“œ] ìˆ˜ì§‘: {len(events)}ê°œ\n")
        return events
    
    # ==================== ìœ í‹¸ë¦¬í‹° ====================
    
    def infer_category(self, title: str) -> str:
        """ì¹´í…Œê³ ë¦¬ ì¶”ë¡ """
        if any(w in title for w in ['ì—¬í–‰', 'í˜¸í…”', 'í•­ê³µ', 'ë¦¬ì¡°íŠ¸']):
            return "ì—¬í–‰"
        elif any(w in title for w in ['ì‡¼í•‘', 'í• ì¸', 'ë°±í™”ì ', 'ë§ˆíŠ¸']):
            return "ì‡¼í•‘"
        elif any(w in title for w in ['ì‹ì‚¬', 'ë ˆìŠ¤í† ë‘', 'ë‹¤ì´ë‹', 'ìŠ¤íƒ€ë²…ìŠ¤', 'ì¹´í˜']):
            return "ì‹ìŒë£Œ"
        elif any(w in title for w in ['ìë™ì°¨', 'ë³´í—˜', 'ì£¼ìœ ', 'ì°¨ëŸ‰']):
            return "êµí†µ"
        elif any(w in title for w in ['ì˜í™”', 'ê³µì—°', 'ë¬¸í™”', 'CGV']):
            return "ë¬¸í™”"
        elif any(w in title for w in ['ê¸ˆë¦¬', 'ëŒ€ì¶œ', 'í• ë¶€', 'ê¸ˆìœµ']):
            return "ê¸ˆìœµ"
        elif any(w in title for w in ['í†µì‹ ', 'ë„·í”Œë¦­ìŠ¤', 'ìœ íŠœë¸Œ', 'êµ¬ë…']):
            return "í†µì‹ "
        else:
            return "ìƒí™œ"
    
    def infer_threat(self, title: str) -> str:
        """ìœ„í˜‘ë„ ì¶”ë¡ """
        if any(w in title for w in ['10ë§Œì›', '20ë§Œì›', '30ë§Œì›', 'ìµœëŒ€', 'í”„ë¦¬ë¯¸ì—„']):
            return "High"
        elif any(w in title for w in ['1ë§Œì›', '2ë§Œì›', '3ë§Œì›', '5ì²œì›']):
            return "Mid"
        else:
            return "Low"
    
    # ==================== ì „ì²´ ì‹¤í–‰ ====================
    
    async def crawl_all(self) -> List[Dict]:
        """ì „ì²´ ì¹´ë“œì‚¬ í¬ë¡¤ë§"""
        print("\n" + "="*70)
        print("ğŸš€ Smart Multi-Strategy Crawler")
        print("="*70 + "\n")
        
        all_events = []
        
        await self.init_browser(headless=True)
        
        try:
            # 1. ì‚¼ì„±ì¹´ë“œ (API)
            samsung_events = await self.crawl_samsung()
            all_events.extend(samsung_events)
            await asyncio.sleep(3)
            
            # 2. ì‹ í•œì¹´ë“œ (HTML)
            shinhan_events = await self.crawl_shinhan()
            all_events.extend(shinhan_events)
            await asyncio.sleep(3)
            
            # 3. í˜„ëŒ€ì¹´ë“œ (íƒ­ + HTML)
            hyundai_events = await self.crawl_hyundai()
            all_events.extend(hyundai_events)
            await asyncio.sleep(3)
            
            # 4. KBêµ­ë¯¼ì¹´ë“œ (ê¸´ ëŒ€ê¸° + HTML)
            kb_events = await self.crawl_kb()
            all_events.extend(kb_events)
            
        finally:
            if self.browser:
                await self.browser.close()
        
        print("="*70)
        print(f"ğŸ‰ ì „ì²´ ìˆ˜ì§‘: {len(all_events)}ê°œ")
        print("="*70 + "\n")
        
        # í†µê³„
        stats = {}
        for e in all_events:
            comp = e['company']
            stats[comp] = stats.get(comp, 0) + 1
        
        for comp, count in stats.items():
            print(f"  - {comp}: {count}ê°œ")
        
        print()
        return all_events
    
    def save_to_db(self, events: List[Dict]):
        """DB ì €ì¥"""
        if not events:
            return
        
        print("="*70)
        print(f"[DB ì €ì¥] {len(events)}ê°œ ì €ì¥ ì¤‘...")
        print("="*70 + "\n")
        
        init_db()
        db = SessionLocal()
        
        try:
            saved = 0
            duplicate = 0
            
            for i, event in enumerate(events, 1):
                success = insert_event(db, event)
                if success:
                    saved += 1
                    print(f"  [{i:3d}] âœ… {event['company'][:4]} - {event['title'][:45]}")
                else:
                    duplicate += 1
            
            print(f"\nì‹ ê·œ: {saved}ê°œ | ì¤‘ë³µ: {duplicate}ê°œ\n")
        
        finally:
            db.close()


async def main():
    """ë©”ì¸"""
    crawler = SmartMultiStrategyCrawler()
    
    # í¬ë¡¤ë§
    events = await crawler.crawl_all()
    
    # ì €ì¥
    crawler.save_to_db(events)
    
    print("\nâœ… ì™„ë£Œ! ëŒ€ì‹œë³´ë“œ: http://localhost:8000\n")


if __name__ == "__main__":
    asyncio.run(main())
