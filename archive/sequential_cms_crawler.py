"""
Sequential CMS ID Crawler
cms_idë¥¼ ìˆœì°¨ ì¦ê°€ì‹œí‚¤ë©° ëª¨ë“  ì‚¼ì„±ì¹´ë“œ ì´ë²¤íŠ¸ ìˆ˜ì§‘

ì „ëµ: cms_idë¥¼ 1ì”© ì¦ê°€ì‹œí‚¤ë©° í˜ì´ì§€ ì ‘ì†
- ìœ íš¨í•œ ì´ë²¤íŠ¸: ì •ìƒ ë¡œë“œ
- ì—†ëŠ” ì´ë²¤íŠ¸: "ì¡°íšŒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤" íŒì—…
"""

import asyncio
import sys
import io
from playwright.async_api import async_playwright
from playwright_stealth import stealth_async
from bs4 import BeautifulSoup
from database import SessionLocal, insert_event, init_db

if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


class SequentialCMSCrawler:
    """ìˆœì°¨ CMS ID í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.base_url = "https://www.samsungcard.com/personal/event/ing/UHPPBE1403M0.jsp?cms_id="
    
    async def check_and_crawl(self, page, cms_id: int) -> dict:
        """ë‹¨ì¼ cms_id í™•ì¸ ë° í¬ë¡¤ë§"""
        url = f"{self.base_url}{cms_id}"
        
        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=15000)
            await asyncio.sleep(2)
            
            # "ì¡°íšŒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤" ì²´í¬
            html = await page.content()
            
            if "ì¡°íšŒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤" in html or "ì—†ìŠµë‹ˆë‹¤" in html and "ì´ë²¤íŠ¸" not in html:
                return None  # ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë²¤íŠ¸
            
            # ìœ íš¨í•œ ì´ë²¤íŠ¸! í¬ë¡¤ë§
            soup = BeautifulSoup(html, 'lxml')
            
            # ì œëª© ì¶”ì¶œ (ì‚¬ì´íŠ¸ í—¤ë” "ì‚¼ì„±ì¹´ë“œ" ë“± ì œì™¸, ë³¸ë¬¸ ì´ë²¤íŠ¸ ì œëª©ë§Œ ì‚¬ìš©)
            title = self._extract_event_title(soup, html)
            
            if not title or len(title) < 3:
                return None
            
            # ê¸°ê°„ ì¶”ì¶œ
            period = "ì •ë³´ ì—†ìŒ"
            period_keywords = ['ê¸°ê°„', 'ì´ë²¤íŠ¸ ê¸°ê°„', 'ì§„í–‰ê¸°ê°„']
            for keyword in period_keywords:
                period_elem = soup.find(string=lambda text: text and keyword in text)
                if period_elem:
                    period_text = period_elem.parent.get_text(strip=True) if period_elem.parent else period_elem
                    period = period_text[:100]
                    break
            
            # í˜œíƒ ë‚´ìš© ì¶”ì¶œ
            benefit = []
            benefit_keywords = ['í˜œíƒ', 'í• ì¸', 'ìºì‹œë°±', 'ì¦ì •', 'ì ë¦½']
            benefit_elems = soup.find_all(string=lambda text: text and any(kw in text for kw in benefit_keywords))
            
            for elem in benefit_elems[:5]:
                text = elem.parent.get_text(strip=True) if elem.parent else elem
                if len(text) > 10 and len(text) < 150:
                    benefit.append(text)
            
            benefit_value = ' / '.join(benefit[:2]) if benefit else "ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°"
            
            # ì¡°ê±´ ì¶”ì¶œ
            condition = []
            condition_keywords = ['ì¡°ê±´', 'ëŒ€ìƒ', 'ì œì™¸', 'ìœ ì˜ì‚¬í•­']
            condition_elems = soup.find_all(string=lambda text: text and any(kw in text for kw in condition_keywords))
            
            for elem in condition_elems[:3]:
                text = elem.parent.get_text(strip=True) if elem.parent else elem
                if len(text) > 10 and len(text) < 150:
                    condition.append(text)
            
            conditions = ' / '.join(condition[:2]) if condition else "ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°"
            
            # ì¹´í…Œê³ ë¦¬ & ìœ„í˜‘ë„ ì¶”ë¡ 
            category = self.infer_category(title)
            threat_level = self.infer_threat(title + ' ' + benefit_value)
            
            # ì „ì²´ í…ìŠ¤íŠ¸
            body_text = soup.get_text(separator='\n', strip=True)
            lines = [l for l in body_text.split('\n') if l.strip()]
            raw_text = '\n'.join(lines)[:2000]
            
            return {
                "url": url,
                "company": "ì‚¼ì„±ì¹´ë“œ",
                "category": category,
                "title": title,
                "period": period,
                "benefit_type": "í• ì¸" if "í• ì¸" in benefit_value else "ìºì‹œë°±" if "ìºì‹œë°±" in benefit_value else "ê¸°íƒ€",
                "benefit_value": benefit_value,
                "conditions": conditions,
                "target_segment": "ì¼ë°˜",
                "threat_level": threat_level,
                "one_line_summary": title,
                "raw_text": raw_text
            }
        
        except Exception as e:
            return None
    
    # ì‚¬ì´íŠ¸ëª…/í—¤ë”ë¡œ ì“¸ ìˆ˜ ìˆëŠ” ë¬¸êµ¬ â†’ ì´ë²¤íŠ¸ ì œëª©ìœ¼ë¡œ ì“°ì§€ ì•ŠìŒ
    _HEADER_LIKE = ('ì‚¼ì„±ì¹´ë“œ', 'ì‚¼ì„± ì¹´ë“œ', 'samsungcard', 'samsung card', 'Samsung', 'SAMSUNG', 'ë¡œê·¸ì¸', 'ë§ˆì´í˜ì´ì§€', 'ì´ë²¤íŠ¸ ëª©ë¡', 'ê°œì¸ì¹´ë“œ', 'ê¸°ì—…ì¹´ë“œ')
    # ì•Œë¦¼/ë°°ë„ˆ ë¬¸êµ¬ â†’ ì‹¤ì œ ì´ë²¤íŠ¸ ì œëª©ì´ ì•„ë‹˜ (ì œì™¸)
    _NOTIFICATION_PREFIXES = ('ì´ë²¤íŠ¸ì— ì‘ëª¨ë˜ì—ˆìŠµë‹ˆë‹¤', 'ì´ë²¤íŠ¸ì— ì‘ëª¨ ë˜ì—ˆìŠµë‹ˆë‹¤', 'ì´ë²¤íŠ¸ì— ì‘ëª¨ëìŠµë‹ˆë‹¤', 'ì´ë²¤íŠ¸ì— ì‘ëª¨ ëìŠµë‹ˆë‹¤')
    
    def _is_header_like(self, text: str) -> bool:
        if not text or len(text) <= 5:
            return True
        t = text.strip()
        for h in self._HEADER_LIKE:
            if t == h or t.startswith(h + ' ') or t.endswith(' ' + h):
                return True
        if t in ('ì´ë²¤íŠ¸', 'í˜œíƒ', 'í”„ë¡œëª¨ì…˜') and len(t) <= 5:
            return True
        return False
    
    def _is_notification_banner(self, text: str) -> bool:
        """'ì´ë²¤íŠ¸ì— ì‘ëª¨ë˜ì—ˆìŠµë‹ˆë‹¤...' ê°™ì€ ì•Œë¦¼ ë¬¸êµ¬ë©´ True â†’ ì œëª© í›„ë³´ì—ì„œ ì œì™¸."""
        if not text or len(text) < 10:
            return False
        t = text.strip()
        if any(t.startswith(p) for p in self._NOTIFICATION_PREFIXES):
            return True
        if 'ë§ˆì´í™ˆ ì•±ì˜' in t and 'ìì‚° ì—°ê²°' in t:
            return True
        return False
    
    def _extract_event_title(self, soup: BeautifulSoup, html: str) -> str:
        """ë³¸ë¬¸ ì´ë²¤íŠ¸ ì œëª©ë§Œ ì¶”ì¶œ (í—¤ë”/ì•Œë¦¼ ë¬¸êµ¬ ì œì™¸). 'ì´ë²¤íŠ¸ì— ì‘ëª¨ë˜ì—ˆìŠµë‹ˆë‹¤' ë“±ì€ ì œì™¸."""
        candidates = []
        
        def _ok(t: str) -> bool:
            if not t or len(t) < 4:
                return False
            if self._is_header_like(t) or self._is_notification_banner(t):
                return False
            return True
        
        # 1) ë³¸ë¬¸ ì˜ì—­ ìš°ì„ : main, .content, .event-detail, [class*="event"], [class*="detail"]
        for scope_sel in ['main', '.content', '.event-detail', '[class*="event"]', '[class*="detail"]', '[class*="campaign"]', 'article']:
            scope = soup.select_one(scope_sel)
            if not scope:
                continue
            for tag in scope.find_all(['h1', 'h2', 'h3']):
                t = tag.get_text(strip=True)
                if _ok(t):
                    candidates.append((len(t), t))
        
        # 2) ì „ì²´ì—ì„œ h1, h2, h3 (í—¤ë”/ì•Œë¦¼ ì œì™¸)
        for tag in soup.find_all(['h1', 'h2', 'h3']):
            t = tag.get_text(strip=True)
            if _ok(t):
                candidates.append((len(t), t))
        
        # 3) í´ë˜ìŠ¤ë¡œ ì œëª©ì¼ ê°€ëŠ¥ì„± ìˆëŠ” ê²ƒ
        for sel in ['.event-title', '.title', '.tit', '.campaign-title', '[class*="tit"]', '[class*="title"]']:
            try:
                el = soup.select_one(sel)
                if el:
                    t = el.get_text(strip=True)
                    if _ok(t):
                        candidates.append((len(t), t))
            except Exception:
                pass
        
        # 4) meta og:title (ì´ë²¤íŠ¸ í˜ì´ì§€ëŠ” ë³´í†µ ì´ë²¤íŠ¸ëª…ì´ ë“¤ì–´ê°)
        try:
            og = soup.find('meta', property='og:title')
            if og and og.get('content'):
                t = og['content'].strip()
                for suffix in (' | ì‚¼ì„±ì¹´ë“œ', ' | Samsung', '- ì‚¼ì„±ì¹´ë“œ', '- Samsung'):
                    if t.endswith(suffix):
                        t = t[:-len(suffix)].strip()
                        break
                if _ok(t):
                    candidates.append((len(t), t))
        except Exception:
            pass
        
        if not candidates:
            return None
        # ê°™ì€ ì œëª© ì¤‘ë³µ ì œê±°, ì•Œë¦¼ ë¬¸êµ¬ ì œì™¸, ì‹¤ì œ ì´ë²¤íŠ¸ ì œëª©(í˜œíƒ/í• ì¸/ë¸Œëœë“œ ë“±) ìš°ì„ 
        seen = set()
        unique = []
        for _, t in sorted(candidates, key=lambda x: (-len(x[1]), x[1])):
            if t in seen or self._is_notification_banner(t):
                continue
            seen.add(t)
            unique.append(t)
        # 'í˜œíƒìœ¼ë¡œ ë§Œë‚˜ ë³´ì„¸ìš”' ë“± ì‹¤ì œ ì´ë²¤íŠ¸ ì œëª© íŒ¨í„´ ìš°ì„  (í˜¼ë‹¤ ìë™ì°¨Â·ë“±ë¡ê¸ˆ ë“±)
        for t in unique:
            if any(k in t for k in ('í˜œíƒ', 'í• ì¸', 'ìºì‹œë°±', 'í”„ë¡œëª¨ì…˜', 'ì„œ.í”„.ë¼', '2ì›”', '3ì›”', 'í• ë¶€', 'ë“±ë¡ê¸ˆ', 'ìë™ì°¨')):
                return t
        return unique[0] if unique else None
    
    def infer_category(self, text: str) -> str:
        """ì¹´í…Œê³ ë¦¬ ì¶”ë¡ """
        if any(w in text for w in ['ì—¬í–‰', 'í˜¸í…”', 'í•­ê³µ']):
            return "ì—¬í–‰"
        elif any(w in text for w in ['ì‡¼í•‘', 'í• ì¸', 'ë°±í™”ì ']):
            return "ì‡¼í•‘"
        elif any(w in text for w in ['ì‹ì‚¬', 'ë ˆìŠ¤í† ë‘', 'ë‹¤ì´ë‹', 'ìŠ¤íƒ€ë²…ìŠ¤', 'ì¹´í˜']):
            return "ì‹ìŒë£Œ"
        elif any(w in text for w in ['ìë™ì°¨', 'ë³´í—˜', 'ì£¼ìœ ']):
            return "êµí†µ"
        elif any(w in text for w in ['ì˜í™”', 'ê³µì—°', 'ë¬¸í™”']):
            return "ë¬¸í™”"
        elif any(w in text for w in ['ê¸ˆë¦¬', 'ëŒ€ì¶œ', 'í• ë¶€']):
            return "ê¸ˆìœµ"
        elif any(w in text for w in ['í†µì‹ ', 'ë„·í”Œë¦­ìŠ¤', 'ìœ íŠœë¸Œ']):
            return "í†µì‹ "
        else:
            return "ìƒí™œ"
    
    def infer_threat(self, text: str) -> str:
        """ìœ„í˜‘ë„ ì¶”ë¡ """
        if any(w in text for w in ['10ë§Œì›', '20ë§Œì›', '30ë§Œì›', '50ë§Œì›', 'ìµœëŒ€', 'í”„ë¦¬ë¯¸ì—„']):
            return "High"
        elif any(w in text for w in ['1ë§Œì›', '2ë§Œì›', '3ë§Œì›', '5ì²œì›']):
            return "Mid"
        else:
            return "Low"
    
    async def crawl_range(self, start_id: int = 3700000, end_id: int = 3750000, batch_size: int = 100):
        """ë²”ìœ„ í¬ë¡¤ë§"""
        print("\n" + "="*70)
        print("ğŸš€ Sequential CMS ID Crawler")
        print(f"   ë²”ìœ„: {start_id} ~ {end_id}")
        print("="*70 + "\n")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
                viewport={'width': 1920, 'height': 1080}
            )
            
            page = await context.new_page()
            await stealth_async(page)
            
            collected_events = []
            consecutive_fails = 0
            max_consecutive_fails = 50  # ì—°ì† 50ë²ˆ ì‹¤íŒ¨í•˜ë©´ ì¢…ë£Œ
            
            current_id = start_id
            
            while current_id <= end_id:
                # batch ë‹¨ìœ„ë¡œ ì§„í–‰ ìƒí™© ì¶œë ¥
                if (current_id - start_id) % batch_size == 0:
                    print(f"\n[ì§„í–‰] cms_id {current_id} ~ {current_id + batch_size - 1} íƒìƒ‰ ì¤‘...")
                    print(f"  í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘: {len(collected_events)}ê°œ")
                
                event_data = await self.check_and_crawl(page, current_id)
                
                if event_data:
                    consecutive_fails = 0
                    collected_events.append(event_data)
                    print(f"  âœ… [{current_id}] {event_data['title'][:50]}")
                    print(f"      ê¸°ê°„: {event_data['period'][:50]}")
                else:
                    consecutive_fails += 1
                
                # ì—°ì† ì‹¤íŒ¨ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì¢…ë£Œ
                if consecutive_fails >= max_consecutive_fails:
                    print(f"\n[INFO] ì—°ì† {max_consecutive_fails}ë²ˆ ì‹¤íŒ¨, íƒìƒ‰ ì¢…ë£Œ")
                    break
                
                current_id += 1
                
                # ë„ˆë¬´ ë¹ ë¥´ì§€ ì•Šê²Œ
                await asyncio.sleep(0.5)
            
            await browser.close()
            
            print(f"\n{'='*70}")
            print(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(collected_events)}ê°œ ì´ë²¤íŠ¸ ë°œê²¬")
            print(f"{'='*70}\n")
            
            return collected_events
    
    def save_to_db(self, events: list):
        """DB ì €ì¥"""
        if not events:
            print("[WARN] ì €ì¥í•  ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"[DB ì €ì¥] {len(events)}ê°œ ì €ì¥ ì¤‘...\n")
        
        init_db()
        db = SessionLocal()
        
        try:
            saved = 0
            duplicate = 0
            
            for i, event in enumerate(events, 1):
                success = insert_event(db, event)
                if success:
                    saved += 1
                    print(f"  [{i:3d}] âœ… {event['title'][:50]}")
                else:
                    duplicate += 1
            
            print(f"\nì‹ ê·œ: {saved}ê°œ | ì¤‘ë³µ: {duplicate}ê°œ\n")
        
        finally:
            db.close()


async def main():
    """ë©”ì¸ ì‹¤í–‰"""
    crawler = SequentialCMSCrawler()
    
    print("\nì‚¼ì„±ì¹´ë“œ ìˆœì°¨ í¬ë¡¤ë§")
    print("="*70)
    print("\nì˜µì…˜:")
    print("  1. ìµœê·¼ ì´ë²¤íŠ¸ (3735000 ~ 3736000) - ë¹ ë¦„")
    print("  2. 2ì›” ì „ì²´ (3730000 ~ 3740000) - ì¤‘ê°„")
    print("  3. 2026ë…„ ì „ì²´ (3700000 ~ 3800000) - ëŠë¦¼ (ì•½ 10-20ë¶„)")
    
    choice = input("\nì„ íƒ (1-3): ").strip()
    
    if choice == '1':
        start, end = 3735000, 3736000
    elif choice == '2':
        start, end = 3730000, 3740000
    elif choice == '3':
        start, end = 3700000, 3800000
    else:
        print("[ERROR] ì˜ëª»ëœ ì„ íƒ")
        return
    
    # í¬ë¡¤ë§
    events = await crawler.crawl_range(start, end, batch_size=100)
    
    # ì €ì¥
    crawler.save_to_db(events)
    
    print("\nâœ… ì™„ë£Œ! ëŒ€ì‹œë³´ë“œ: http://localhost:8000\n")


if __name__ == "__main__":
    asyncio.run(main())
