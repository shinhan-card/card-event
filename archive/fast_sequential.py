"""
ë¹ ë¥¸ ìˆœì°¨ í¬ë¡¤ëŸ¬ - ì‹¤ì „ ë²„ì „
cms_id 3733000~3736000 íƒìƒ‰ (ìµœê·¼ ì´ë²¤íŠ¸ ì¤‘ì‹¬)
"""

import asyncio
import sys
import io
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from database import SessionLocal, insert_event, init_db

if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


async def crawl():
    print("\nğŸš€ ë¹ ë¥¸ ìˆœì°¨ í¬ë¡¤ë§ ì‹œì‘\n")
    
    base = "https://www.samsungcard.com/personal/event/ing/UHPPBE1403M0.jsp?cms_id="
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        found = []
        fails = 0
        
        # ë” ì¢ì€ ë²”ìœ„ë¡œ ë¹ ë¥´ê²Œ
        for cms_id in range(3733000, 3736000):
            url = f"{base}{cms_id}"
            
            try:
                resp = await page.goto(url, timeout=5000)
                await asyncio.sleep(0.3)
                
                html = await page.content()
                
                if "ì¡°íšŒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤" in html or resp.status == 404:
                    fails += 1
                    if fails >= 30:
                        print(f"\nì—°ì† 30ë²ˆ ì‹¤íŒ¨, ì¢…ë£Œ (í˜„ì¬ {len(found)}ê°œ)\n")
                        break
                    continue
                
                fails = 0
                soup = BeautifulSoup(html, 'lxml')
                
                # ì œëª©
                title = None
                for h in soup.find_all(['h1', 'h2']):
                    t = h.get_text(strip=True)
                    if 5 < len(t) < 80:
                        title = t
                        break
                
                if not title:
                    continue
                
                # ê¸°ê°„
                import re
                period = "ì •ë³´ ì—†ìŒ"
                text = soup.get_text()
                match = re.search(r'20\d{2}\.\d{2}\.\d{2}~20\d{2}\.\d{2}\.\d{2}', text)
                if match:
                    period = match.group()
                
                # ì¹´í…Œê³ ë¦¬
                cat = "ìƒí™œ"
                if 'ì—¬í–‰' in title: cat = "ì—¬í–‰"
                elif 'ì‡¼í•‘' in title: cat = "ì‡¼í•‘"
                elif 'ì‹ì‚¬' in title or 'ë‹¤ì´ë‹' in title: cat = "ì‹ìŒë£Œ"
                elif 'ìë™ì°¨' in title or 'ë³´í—˜' in title: cat = "êµí†µ"
                elif 'ê¸ˆë¦¬' in title or 'í• ë¶€' in title: cat = "ê¸ˆìœµ"
                
                found.append({
                    "url": url,
                    "company": "ì‚¼ì„±ì¹´ë“œ",
                    "category": cat,
                    "title": title,
                    "period": period,
                    "benefit_type": "í˜œíƒ",
                    "benefit_value": "ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°",
                    "conditions": "ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°",
                    "target_segment": "ì¼ë°˜",
                    "threat_level": "Mid",
                    "one_line_summary": title,
                    "raw_text": title
                })
                
                print(f"âœ… [{cms_id}] {title[:50]}")
            
            except:
                fails += 1
                if fails >= 30:
                    break
        
        await browser.close()
        
        print(f"\nì´ {len(found)}ê°œ ë°œê²¬!\n")
        return found


def save(events):
    if not events:
        return
    
    print(f"DB ì €ì¥ ì¤‘...\n")
    
    init_db()
    db = SessionLocal()
    
    try:
        saved = 0
        for e in events:
            if insert_event(db, e):
                saved += 1
        
        print(f"ì‹ ê·œ: {saved}ê°œ\n")
    finally:
        db.close()


async def main():
    events = await crawl()
    save(events)
    print("âœ… ì™„ë£Œ! http://localhost:8000\n")


if __name__ == "__main__":
    asyncio.run(main())
