"""
Simple Final Crawler - ë‹¨ìˆœí•˜ê³  í™•ì‹¤í•œ í¬ë¡¤ëŸ¬
ë³µì¡í•œ ê¸°ëŠ¥ ì œê±°, ì‹¤ìš©ì„±ì— ì§‘ì¤‘

ëª©í‘œ:
1. ì‚¼ì„±ì¹´ë“œ cms_id ìˆœì°¨ í¬ë¡¤ë§ (2026ë…„ ëª¨ë“  ì´ë²¤íŠ¸)
2. ì œëª©, ê¸°ê°„, ê°„ë‹¨í•œ í˜œíƒë§Œ ì •í™•í•˜ê²Œ ìˆ˜ì§‘
3. ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ ì‘ë™
"""

import asyncio
import sys
import io
from playwright.async_api import async_playwright
from playwright_stealth import stealth_async
from bs4 import BeautifulSoup
from database import SessionLocal, insert_event, init_db
from datetime import datetime

if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


async def crawl_samsung_simple(start_id: int, end_id: int):
    """ì‚¼ì„±ì¹´ë“œ ì‹¬í”Œ í¬ë¡¤ëŸ¬"""
    
    print("\n" + "="*70)
    print(f"ğŸš€ ì‚¼ì„±ì¹´ë“œ ì‹¬í”Œ í¬ë¡¤ëŸ¬")
    print(f"   ë²”ìœ„: {start_id} ~ {end_id}")
    print(f"   ì‹œì‘: {datetime.now().strftime('%H:%M:%S')}")
    print("="*70 + "\n")
    
    base_url = "https://www.samsungcard.com/personal/event/ing/UHPPBE1403M0.jsp?cms_id="
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        await stealth_async(page)
        
        events = []
        fails = 0
        
        for cms_id in range(start_id, end_id + 1):
            if (cms_id - start_id) % 50 == 0:
                print(f"[{cms_id}] íƒìƒ‰ ì¤‘... (í˜„ì¬ {len(events)}ê°œ ë°œê²¬)")
            
            url = f"{base_url}{cms_id}"
            
            try:
                await page.goto(url, timeout=8000)
                await asyncio.sleep(0.5)
                
                html = await page.content()
                
                # ì¡°íšŒ ê²°ê³¼ ì—†ìŒ ì²´í¬
                if "ì¡°íšŒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤" in html:
                    fails += 1
                    if fails >= 50:
                        print(f"\nì—°ì† 50ë²ˆ ì‹¤íŒ¨, ì¢…ë£Œ")
                        break
                    continue
                
                fails = 0
                soup = BeautifulSoup(html, 'lxml')
                
                # ì œëª©
                title = None
                for tag in soup.find_all(['h1', 'h2', 'h3']):
                    text = tag.get_text(strip=True)
                    if 5 < len(text) < 100:
                        title = text
                        break
                
                if not title:
                    continue
                
                # ê¸°ê°„ ì°¾ê¸° (ê°„ë‹¨í•˜ê²Œ)
                period = "ì •ë³´ ì—†ìŒ"
                full_text = soup.get_text()
                
                # "2026.02.01~2026.02.28" íŒ¨í„´ ì°¾ê¸°
                import re
                date_pattern = r'20\d{2}\.\d{2}\.\d{2}[~\-]20\d{2}\.\d{2}\.\d{2}'
                match = re.search(date_pattern, full_text)
                if match:
                    period = match.group(0).replace('-', '~')
                
                # ì¹´í…Œê³ ë¦¬ (ê°„ë‹¨í•˜ê²Œ)
                category = "ê¸°íƒ€"
                if 'ì—¬í–‰' in title: category = "ì—¬í–‰"
                elif 'ì‡¼í•‘' in title or 'í• ì¸' in title: category = "ì‡¼í•‘"
                elif 'ì‹ì‚¬' in title or 'ë‹¤ì´ë‹' in title or 'ìŠ¤íƒ€ë²…ìŠ¤' in title: category = "ì‹ìŒë£Œ"
                elif 'ìë™ì°¨' in title or 'ë³´í—˜' in title: category = "êµí†µ"
                elif 'ì˜í™”' in title or 'ê³µì—°' in title: category = "ë¬¸í™”"
                elif 'ê¸ˆë¦¬' in title or 'í• ë¶€' in title: category = "ê¸ˆìœµ"
                
                events.append({
                    "url": url,
                    "company": "ì‚¼ì„±ì¹´ë“œ",
                    "category": category,
                    "title": title,
                    "period": period,
                    "benefit_type": "í˜œíƒ",
                    "benefit_value": "ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°",
                    "conditions": "ìƒì„¸ í˜ì´ì§€ ì°¸ì¡°",
                    "target_segment": "ì¼ë°˜",
                    "threat_level": "Mid",  # ê¸°ë³¸ê°’
                    "one_line_summary": title,
                    "raw_text": title
                })
                
                print(f"  âœ… [{cms_id}] {title[:50]}")
            
            except:
                fails += 1
                if fails >= 50:
                    break
        
        await browser.close()
        
        print(f"\n{'='*70}")
        print(f"ì™„ë£Œ: {len(events)}ê°œ ìˆ˜ì§‘")
        print(f"ì‹œê°„: {datetime.now().strftime('%H:%M:%S')}")
        print(f"{'='*70}\n")
        
        return events


def save_events(events):
    """DB ì €ì¥"""
    if not events:
        return
    
    print(f"[DB ì €ì¥] {len(events)}ê°œ ì €ì¥ ì¤‘...\n")
    
    init_db()
    db = SessionLocal()
    
    try:
        saved = 0
        for event in events:
            if insert_event(db, event):
                saved += 1
                print(f"  âœ… {event['title'][:50]}")
        
        print(f"\nì‹ ê·œ ì €ì¥: {saved}ê°œ\n")
    finally:
        db.close()


async def main():
    """ë©”ì¸ - ì‚¼ì„±ì¹´ë“œ 2ì›” ì§‘ì¤‘"""
    # 2026ë…„ 2ì›” ì¤‘ì‹¬ ë²”ìœ„
    events = await crawl_samsung_simple(3733000, 3737000)
    save_events(events)
    print("\nâœ… ì™„ë£Œ! http://localhost:8000\n")


if __name__ == "__main__":
    asyncio.run(main())
